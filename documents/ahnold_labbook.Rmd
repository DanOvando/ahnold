---
title: "ahnold_labbook"
author: "Dan Ovando"
date: "December 14, 2016"
output: 
  html_document:
    toc: yes
    toc_float: yes
---

```{r}
knitr::opts_chunk$set(eval = F, echo = F, message = F)
```


```{r libraries, echo=F, message=F, warning=F, include=F}
library(tidyverse)
library(forcats)
library(rstanarm)
library(lubridate)
library(ggsci)
library(broom)
library(car)
library(stringr)
library(rvest)

demons::load_functions(func_dir = '../functions')

```

A running labbook for ideas in the Ahnold project

## Bioregions

Looking at @Hamilton2010, bioregions clearly play a role. You could include them simply as covariates. But, that doesn't account for the potential interactions between MPAs and regions. Might need to either run them as seperate models (worth it to see), with interaction effects with MPA to see if there are differences among bioregions in MPA effects

## Interspecies Interactions

Can you look at the spatial-covariance matrix of these species?

Or is there enough data to sugihara the damn thing, and ask whether some species seem to cause decreases in others?

## Landings data

*[Can be found here](https://www.wildlife.ca.gov/Fishing/Commercial/Landings#260041375-2015)
* Can pretty easily figure out a way to scrape all this


## Fixed effects, random effects, and errors in variables, and clustering

It's important that you get this straightened out. 

The "errors in variables" relates to the idea that your covariate data are now random draws from some distribution. So, for example, you observe length data, which you fit to a model that predicts length data and a standard deviation, and you pass those predicted length data to some other part of the model. See box 6.2.2 in bayesian primer. This is just basically saying as another random variable in the liklihood, instead of taking it as given (where you would just use the length data)

Basically, what I'm trying to figure out here is how to properly deal with the the clustering of the data in the time series. So, for any categorical variable that is at a higher level than the data, it seems reasonable to just give it a prior (e.g. all the year coefficients share a common prior), all the species effects share a prior by group, etc, where all of those share a common prior. 

What do you do about things like linf though? Repeated a whole ton over time, but you can't just give it a hierarchical prior.... Looking at Gelman and Hill


OOOOOOOHHHHHH, maybe this is the right approach. Give it species fixed effects, and then make the species fixed effects a regression of species effects coefficients on life history traits. So, a multi-level modeling approach, like Gelman and Hill page 241. 

From a hierarchichal perspective then, the idea is now to think of these as a bunch of nested regressions for each level of the data, with the right random effects linking each layer together. 

So, you have year terms, and those year terms are a function of yearly things like temperature and el ni√±o, species terms, etc. The problem that you've been having, and this probably explains some of the convergance issues, is that you have massive colinearity when you try and include fixed effects for things like species, and then the covariates at the species level? So, this allows you to sort of get the best of both worlds, where you now get the "intercepts" by the right thing, but those intercepts take into account the data at the intercept level that you have that can influence the outcome. So it looks though from Gelman and Hill page 281 that to do this, you don't have to include those terms in the mixed effects part of the regression itself, but only put them in, but with the appropriate priors. So, that's where the common prior comes in? You'll just need to mess with this one with code once you can code it in STAN

You should test this by getting the thing written in STAN, and comparing two versions, one where you do it the usual way (estimate the mean of the prior), one where you include covariates in the model, the other where you fit them seperately

  AHA@@!!@!#@!@#! THAT'S WHY THE MEAN OF THE PRIOR IS 0!!!! think about it. Say you want to include fixed effects for each site. Under the model as it get's written all over the place here, they are distributed with mean(u,sigma). Now, when you include the fixed effects mean in that context: those are deviations from a mean 0! So, you've already accounted for the change from the mean effect by including the intercept. If you then say include site specific temperature in there, you'd just chuck it in as a variable, which would do the exact same thing as saying N(site + temp,sigma), or site + temp + n(0,sigma). All those terms are getting soaked up in the intercept. Suppose that it should be N(.2,sigma). So, no matter what, ever year term is going to be .2 +- some term. That is the same then as just adding .2 to the intercept, and then setting the mean of the prior to 0. So, they real key here then is simply specifying the group-level standard deviation. Wow. 



**Everything is a random effect in bayes, the key is is it grouped**

## Model Structure

Let's go through the thing from scratch and think through how you want this thing to look, from a model diagramming perspective. 

### Hurdle component

### Data

So may be you need to be thinking about this kind of like a CPUE standardization, where on one hand you're fitting a poisson distribution to the transect data, and using that to estimate densities, based on deterministic equations and a sigma (or maybe treating that part as data to make things a bit easier), and then fitting the regression to those densities. The challenging part is how you rationalize the two distributions for density; that implied by the regressin and that implied by the error from the logit model 

AHA, so the key here is thinking about the marginal posterior of the regression coefficients, not just the regression coefficients. The sigma of the regression model us saying taking those data as observations from a ranfom event, how well does the model explain that. But, that's just under that one scenario. So, if you were doing this in a simple way where the posterior is analytical, then you get the betas out. But, the full marginal now would take into account the range of worlds that the densities could live in, so you'd sample from the joint posterior to get the betas of interest!

### Process

See box 6.2.2 in a Bayesian Primer for ideas on this. 

The way I'm leaning towards is something like this: There is a true biomass that is a random draw from a distribution with mean observed biomass and a variance defined by.... something (observer, group, a function of kelp, vis, etc.). The inverse of that, that the observed are a random draw from some true distribution with mean true and sigma above seems hard to do in this context: you'd have to fit a massive number, or possibly an unidentifiable number, of means, unless you want to say that that there's some clustering (e.g. mean by species by site by year, which I think are the raw aggregation anyways). 

Ah wait, you're making this wayyyy to complicated. The thing you're thinking about here are errors in **predictor** variables, not dependent variables like the density, which is what you're trying to explain. Under a traditional Bayesian framework, the dependent variabels are already considered random draws (until they are obsered), while the rest are considered data. But, the stuff you were thinking of there are for independent variables, things like temperature. So, you could say that temperature is imperfectyl measured, and comes from a distribution with some true mean and sigma. 

So, you don't need to mess with the "errors" in density, since it's already considered a random draw, from a distribution with mean expeced density from the model itself, and a sigma estiamted by the model. 

Now, where things could then get interesting is using a model to estimate the sigmas. i.e. sigma is a linear function of say a constant, plus kelp cover + vis + research group... etc. 

Now, that's all assuming that the biomass densities are in fact your data. There's another way you could do this, somewhat similar to @Karnauskas2011. The idea here. you have **two**ish data sources. One are the actual lengths. The other are the observed densities. The goal is still to fit a regression to the densities. The issue now though is that you're going to fit the regression to densities estimated from the raw length data. So, you'd take the length data. You'd then use that length data to build up a model of densities that you then fit the regression on. The problem is, you'd need a model relating something back to the length data, which you aren't really doing here. i.e what would the lengths be conditional on? Alternatively, you treat the lengths as data, and the densities as random variables. Now, you have the observed densities, that are draws from a distribution with mean of predicted density, where predicted density is a function of the lengths? That is a bit of double dipping though. Now one option could be to train the regression on "density" data generated from the lengths. So now, you say the observed densities are drawn from a distribution predicted by the regression, where the regression is trained on the expected densities predicted from the length data, estimating sigmas all the way. This seems maybe the most robust option, but also a royal pain in the ass. And do you really gain a whole lot from that?

It seems like you have something like three different model structures all models can share a common hierarchical nature to the data, the question really is how do you want to model errors, especially conditional on what you really care about are doing a good job on the estimators, not on the out-of-sample prediction

1. The standard model, with potential for clustering of errors in densities as a function of appropriate levels (e.g. species groups, etc. ).That's really what equation 6.2.51 in BP is doing: making a more complex model of the error structure associated with the observations. 

2. A model for standard deviations in the densities, where sigma is a linear function of an intercept and apprpriate covariates (like visibility, kelp, etc.) Could be an interesting way to test things that need to be accounted for in the model. That's really what equation 6.2.51 in BP is doing: making a more complex model of the error structure associated with the observations. 

3. A data-generation model, where the length frequency are taken as perfectly observed, or are used to generate draws from a poisson, a la @Karnauskas2011, converted to densities, and then the regression is fit to those densities, and then used to predict the observed densities. Seems like a worse and worse idea, especially if you're not as interested in prediction as you are in estimation.

### Parameters

Let's think about the covariates first. The question is basically how do you want to cluster the errors. 
#### Sites

You have site specific parameters, like location, region, etc. These stay constant at each site over time. Those should almost certainly be modeled as random effects, where maybe each site gets an intercept, drawn from the region that it's in, and each region then get's an uninformative prior, since the region's aren't really random effects, but constant enities in this world.

Now one question I have then is how does that fit in a regression framework, i.e. relative to "controlling" for the region effect in the regression itself. I don't think that it matters, you just need to stop thinking abotu things so linearly. You're only goal is to model the variation in density as a function of things, and this goes in there, the effect of region just isn't a linear additive term, but rather affects the site specific terms

### Species

You also have a bunch of species-specific effects. Now, on one level, these make more sense to me as fixed effects (the idea is that they aren't really random draws from a population, but rather "true" values). But, where I get confused then is how to deal with these correctly in a panel framework, given that they are repeated. If I remember my Gelman, the way to deal with this in the year terms for example is by giving them a hierarchichal nature. 

So, maybe one way to think of all the life history things is that each life history value is a draw from a "random" model that genrates life history traits. So, you estimate the coefficients for linf, where linf is a random variable drawn a global distribution. 

AHA, i think your confusion is in differentiating the data from the coefficients. I think the "random effects" vs "fixed effects" question relates to how you want to model the data itself, as either the data, or draws from some distribution that you then estimate as well. 

"In Bayesian hierarchical modeling, random effects are used to describe variation that occurs beyond variation that arises from sampling alone"

Hobbs, N. Thompson; Hooten, Mevin B.. Bayesian Models: A Statistical Primer for Ecologists (Page 114). Princeton University Press. Kindle Edition. 


### Priors


## Jan 3 2017

### New thoughts on model structure

OK, after doing some reading I think I'm geting a better handle on this. The key question: how do you properly account for the sampling nature of the MLPA data


The problem is you've been thinking about this rather confusedly between the unclear distinction between hierarchical and state space (see your evernore entry on state space modeling). 

So let's start from the ground up. 

1. You observe samples of length data, binned by size
2. These sample arrise from a underlying true length structure, let's say a Poisson, meaning that the mean and SD are the same. So, you fit a model that replicates this process WRONG!!!
    * The samples are the data! they are the mean values of the true poisson distribution. So, if that's the case, none of this matters ha, since there's nothing to estimate. 


If that's the case, maybe you go back to an explicit observation model, where the error in the length structures/biomass are a function of covariates. 



    
```{r}

library(tidyverse)

huh <- rpois(1000,100)

data_frame(huh = huh) %>% 
  ggplot(aes(huh)) + 
  geom_bar()

rmultinom(10, size = 12, prob = c(0.1,0.2,0.8,.1))

```

    
## 1/4/17

OK, that was a good road to head down, but not a great place to start. It seems like there's not really a huge upside to be had from the crazy length-up modeling process. At the end of the day, unless you introduce a bias term or something like that, you're just going to get the mean of the prediction back. So, you could certainly introduce a bit more uncertainty, but not really a substantive change in the outcome. 

```{r}
library(tidyverse)
dat <- read_csv('../data/UCSB_FISH raw thru 2013.csv')

dat <-  dat %>% 
  mutate(size_range = ifelse(is.na(max_tl - min_tl), 0, max_tl - min_tl ))

dat %>% 
  ggplot(aes(size_range)) +
  geom_histogram() + 
  scale_y_log10()

```
Almost all the observations have no real range to them. Looking at a histogram of the range of the sizes reported (max - min), where 0 corresponds to no range reported, almost all are within a few centimeters, which really isn't going to play our in the biomass all that much. only % of samples have size ranges above 10 centimeters. So, to be really really precise, you could simulate length frequences for those, but really hardly seems worth the effort. 

The bigger question then is if you want to try and model bias in the observations; e.g. counts of certain species really should be higher/lower under different conditions. Something like a "standardized" survey index. i.e. maybe counts should be higher, taking into account poor vis or the like. Basically, it will be important if it introduces bias, or there is substantially more error around particular kinds of species. But beyond that, not like your analysis is fundamentally flawed or anything, and it's unclear how much benefit you get from controlling for those factors at the ground level, vs. controlling for them in the regression itself (i.e. all else being equal visibility drives down/up counts). 

There's still the biomass issue (converting lengths into biomass). There's obviously error in there, but probably not bias, so again, the issue would really be in just adding more noise. It's hard to know how you'd deal with that since there's not really a signal in the data to tell you anything about that error. It would be nice to work with Jen to do the translation from lengths to densities right there in the model, but again that seems like a second order thing: I don't think the existing densities are wrong or anything. The biggest thing though is that doing the densities raw would allow you to actually tally them up at the aggregation level you want, instead of taking means/medians across space and time, which could disguise some trends. Let's work on that actually. 

So, I think you're back go the important thing being doing a good job of dealing with the hierarchical nature of the data themselves. So let's spend the next few days focusing on that. 

## Data Exploration

Let's go back to square one and think a little about the nature of the data that you're dealing with, and explore the relationships of the data with density and length. 

```{r load data}


length_data <- read_csv('../data/UCSB_FISH raw thru 2013.csv') %>% 
    magrittr::set_colnames(.,tolower(colnames(.)))


conditions_data <- length_data %>% 
  group_by(site,side,year) %>% 
  summarise(mean_temp = mean(temp, na.rm = T),
            mean_kelp = mean(pctcnpy, na.rm = T),
            mean_vis = mean(vis, na.rm = T)) 
  

life_history_data <- read_csv('../data/VRG Fish Life History in MPA_04_08_11_12 11-Mar-2014.csv') %>%
  rename(classcode = pisco_classcode) %>% 
  magrittr::set_colnames(.,tolower(colnames(.)))

site_data <- read_csv('../data/Final_Site_Table_UCSB.csv') %>%
    magrittr::set_colnames(.,tolower(colnames(.)))


length_data <- length_data %>% 
  left_join(life_history_data, by = 'classcode') %>% 
  left_join(site_data, by = c('site','side'))

```

## Converting length to biomass/density

One thing I'd like to be able to do is move directly from the lengths to the density at any aggregation that I'm interested in by actually tallying the length structure. As it stands right now, you need to take means/medians to aggregate densities over time, which I don't really like

Let's take a look at the actual density data that Jen provides

```{r density data}

density_data <- read_csv('../data/ci_reserve_data_final3 txt.csv') %>%
  magrittr::set_colnames(.,tolower(colnames(.))) %>% 
  gather('concat.name','value', grep('_',colnames(.)),convert = T) %>%
  mutate(data.type = gsub('\\_.*', '', concat.name),
         classcode = gsub('.*\\_','',concat.name)) %>%
  mutate(value = as.numeric(value)) %>%
  spread(data.type,value) %>%
  rename(site_side = site.side)

density_example <- density_data %>% 
  filter(is.na(biomass) == F & biomass >0) %>% 
  sample_n(1)

density_example

```
Let's take a look and see if you can replicate this density example. 





```{r}


length_to_weight <-
  function(mean_length,
  min_length,
  max_length,
  count,
  weight_a,
  weight_b,
  length_units = 'cm',
  weight_units = 'g',
  length_for_weight_units = 'mm',
  length_type_for_weight,
  tl_sl_a,
  tl_sl_b,
  tl_sl_type,
  tl_sl_formula) {
  #
  # generate_lengths <- function(count,mean_length, min_length, max_length){
  
  if (is.na(count) | count == 0){
    
    outweight <-  0
  }  else {
    
  if (is.na(min_length) |
  is.na(max_length)) {
  #generate distribution of lengths
  
  lengths <-  rep(mean_length, count)
  
  } else{
  # lengths <-  pmax(min_length,pmin(max_length,rpois(count, lambda = mean_length)))
  lengths <- runif(count, min = min_length, max = max_length)
  }

  if (length_type_for_weight == 'SL') {
  if (tl_sl_type  == 'TYPICAL') {
  weight_lengths <-  lengths * tl_sl_a + tl_sl_b
  } else{
  weight_lengths <- (lengths - tl_sl_b) / tl_sl_a
  
  }
  
  } else {
  weight_lengths <-  lengths
  }
  
  if (length_units == 'cm' & length_for_weight_units == 'mm') {
  weight_lengths <- weight_lengths * 10
  }
  
  weight <-  weight_a * weight_lengths ^ weight_b
  
  if (weight_units == 'kg') {
  weight <- weight * 1000
  }
  outweight = sum(weight)
  }
    return(outweight)
  } #close function
  
length_example <- length_data %>% 
  filter(classcode == toupper(density_example$classcode)
, site == density_example$site, 
         side == density_example$side, year == density_example$year) 

length_example <-   length_data %>% 
  filter(is.na(commonname) == F) %>% 
  mutate(biomass_g = pmap_dbl(list(mean_length = fish_tl,
                                      min_length = min_tl,
                                      max_length = max_tl,
                                      count = count,
                                      weight_a = wl_a,
                                      weight_b = wl_b,
                                      length_type_for_weight = wl_input_length,
                                      length_for_weight_units = wl_l_units,
                                      tl_sl_a = lc.a._for_wl,
                                      tl_sl_b = lc.b._for_wl,
                                      tl_sl_type = lc_type_for_wl,
                                      tl_sl_formula = ll_equation_for_wl), length_to_weight))

length_example %>% 
  select(biomass_g)

biomass_data <- length_example %>% 
  group_by(classcode, site, side, year, transect) %>% 
  summarise(total_biomass_g = sum(biomass_g)) 
```
First, need to add back in zeros. You need a function that goes through trip by trip, and adds in zeros for all species seen at that site at some point but not on that trip. 


```{r}

species_sightings <- length_data %>% 
  group_by(site) %>% 
  summarise(species_seen = list(unique(classcode)))


biomass_data <- biomass_data %>% 
  ungroup() %>% 
  select(site,side,year, transect) %>% 
  unique() %>%  {
  pmap(
    list(
      this_site = .$site,
      this_side = .$side,
      this_year = .$year,
      this_transect = .$transect
    ),
    add_missing_fish,
    observations = biomass_data,
    species_sightings = species_sightings
  )
} %>% 
  bind_rows()

man_density_data <- biomass_data %>% 
  group_by(classcode, site, side,year) %>% 
  summarise(mean_biomass_g = mean(total_biomass_g, na.rm = T)) %>% 
  mutate(
            biomass_g_per_m2 = mean_biomass_g / (30*4),
            biomass_g_per_hectare = biomass_g_per_m2 * 10000,
            biomass_ton_per_hectare = biomass_g_per_hectare * 1e-6)


```


```{r}


```



OK! You've got hand calculated densities now, let's compare them to jens

```{r}

density_comp_plot <- density_data %>% 
  select(classcode, site, side, year, biomass) %>% 
  mutate(classcode = toupper(classcode)) %>% 
  left_join(man_density_data, by = c('classcode','site','side','year')) %>% 
  left_join(life_history_data, by = 'classcode') %>% 
  ggplot(aes(biomass,biomass_ton_per_hectare, color = commonname)) + 
             geom_abline(aes(intercept = 0, slope = 1), linetype= 2) +

           geom_point() + 
  scale_color_discrete(guide = F) + 
  labs( y = 'Biomass (tons per hectare) - calculated from lengths and weights', x = 'Biomass (tons per hectare) - from ci_reserve_data_final3 txt.csv', 
  caption = 'Resolution is at species-year-site-side')

density_comp_plot
ggsave(density_comp_plot, file = 'density comparison plot.pdf',dev = cairo_pdf)
# plotly::ggplotly(density_comp_plot)

```


Not Bad. It's a good starting point, and makes me confident that I'm understanding things right. You'll need to talk with Jen to figure out why this isn't 1:1. This also let's you compare outcomes under the two sources. 
## Exploring relationships

Let's dig into things a bit here and just look at trends in the (to start with) two versions of the database

```{r}

man_density_data <- man_density_data %>% 
  left_join(life_history_data, by = 'classcode') %>% 
  left_join(site_data, by = c('site','side'))

man_density_data %>% 
  filter(is.na(targeted) == F) %>% 
  group_by(region, year, targeted) %>% 
  summarise(median_biomass = mean(biomass_ton_per_hectare, na.rm = T)) %>% 
  ggplot(aes(year,median_biomass, color = targeted)) + 
  geom_line() + 
  facet_wrap(~region)


```

Now same thing, but with Jen's data

```{r}
density_data %>% 
  mutate(classcode = toupper(classcode)) %>% 
    left_join(life_history_data, by = 'classcode') %>% 
  filter(is.na(targeted) == F) %>% 
  group_by(region, year, targeted) %>% 
  summarise(mean_biomass = mean(biomass, na.rm = T)) %>% 
  ggplot(aes(year,mean_biomass, color = targeted)) + 
  geom_line() + 
  facet_wrap(~region)
```



And just a really quick regression exploration

```{r}



reg_data <- density_data %>% 
  mutate(classcode = toupper(classcode)) %>% 
    left_join(life_history_data, by = 'classcode') %>% 
      left_join(conditions_data, by = c('site','side','year')) %>% 
  filter(is.na(targeted) == F) %>% 
  filter(biomass > 0) %>% 
  mutate(log_biomass = log(biomass),
         mlpa_in_effect = as.numeric(year > 2003),
         fished = as.numeric(targeted == 'Targeted'),
         did = as.numeric(fished * year * mlpa_in_effect))

reg_fmla <- as.formula('log_biomass ~  as.factor(year) + fished +  as.factor(did) + trophicgroup + vbgf.linf +
                       mean_temp')


basic_reg <- lm(reg_fmla, data = reg_data)

# stan_reg <- rstanarm::stan_glm(reg_fmla, data = reg_data)

summary(basic_reg)

broom::tidy(basic_reg)

  basic_reg %>%
    broom::tidy() %>%
    filter(str_detect(term,'did')) %>%
    mutate(year = as.numeric(str_replace(term,'as.factor\\(did\\)','')) )%>%
    ggplot() +
    geom_hline(aes(yintercept = 0)) + 
    geom_pointrange(aes(x = year, y = estimate, ymin = estimate - 1.96*std.error, ymax = estimate + 1.96 * std.error)) +
    geom_smooth(aes(x = year, y = estimate), method = 'lm')

  
    # stan_reg %>%
    # broom::tidy() %>%
    # filter(str_detect(term,'did')) %>%
    # mutate(year = as.numeric(str_replace(term,'as.factor\\(did\\)','')) )%>%
    # ggplot() +
    # geom_pointrange(aes(x = year, y = estimate, ymin = estimate - 1.96*std.error, ymax = estimate + 1.96 * std.error)) + 
    #       geom_smooth(aes(x = year, y = estimate), method = 'lm')



```


```{r}

reg_data <- man_density_data %>% 
  ungroup() %>% 
  filter(is.na(targeted) == F) %>% 
  filter(biomass_ton_per_hectare > 0) %>% 
  mutate(log_biomass = log(biomass_ton_per_hectare),
         mlpa_in_effect = as.numeric(year > 2003),
         fished = as.numeric(targeted == 'Targeted'),
         did = as.numeric(fished * year * mlpa_in_effect))

reg_fmla <- as.formula('log_biomass ~  as.factor(year) + fished +  as.factor(did) + trophicgroup + vbgf.linf')


basic_reg <- lm(reg_fmla, data = reg_data)

# stan_reg <- rstanarm::stan_glm(reg_fmla, data = reg_data)

# summary(basic_reg)
# 
# broom::tidy(basic_reg)

  basic_reg %>%
    broom::tidy() %>%
    filter(str_detect(term,'did')) %>%
    mutate(year = as.numeric(str_replace(term,'as.factor\\(did\\)','')) )%>%
    ggplot() +
    geom_hline(aes(yintercept = 0)) + 
    geom_pointrange(aes(x = year, y = estimate, ymin = estimate - 1.96*std.error, ymax = estimate + 1.96 * std.error)) +
    geom_smooth(aes(x = year, y = estimate), method = 'lm')

  
    # stan_reg %>%
    # broom::tidy() %>%
    # filter(str_detect(term,'did')) %>%
    # mutate(year = as.numeric(str_replace(term,'as.factor\\(did\\)','')) )%>%
    # ggplot() +
    # geom_pointrange(aes(x = year, y = estimate, ymin = estimate - 1.96*std.error, ymax = estimate + 1.96 * std.error)) + 
    #       geom_smooth(aes(x = year, y = estimate), method = 'lm')



```


Well that's not encouraging: this suggests that the density calculation really does matter here. You'll go with Jen's data for now, but big red flag of something that needs fixing here. 



## Are unfished and fished valid controls?

There are a few ways you could think about testing for this. 

1. Does the MLPA come out as a causal factor on the unfished species?

2. Do fished speices/unfished species cause each other (do lags of fished species predict unfished and vice versa)



```{r}
reg_data <- density_data %>% 
  mutate(classcode = toupper(classcode)) %>% 
    left_join(life_history_data, by = 'classcode') %>% 
  left_join(conditions_data, by = c('site','side','year')) %>% 
  filter(is.na(targeted) == F) %>% 
  filter(biomass > 0) %>% 
  mutate(log_biomass = log(biomass),
         mlpa_in_effect = as.numeric(year > 2003),
         unfished = as.numeric(targeted != 'Targeted'),
         did = as.numeric(unfished * year * mlpa_in_effect))

reg_fmla <- as.formula('log_biomass ~  as.factor(year) + mean_temp+unfished +  as.factor(did) + trophicgroup + vbgf.linf')


basic_reg <- lm(reg_fmla, data = reg_data)

# stan_reg <- rstanarm::stan_glm(reg_fmla, data = reg_data)

summary(basic_reg)

broom::tidy(basic_reg)

  basic_reg %>%
    broom::tidy() %>%
    filter(str_detect(term,'did')) %>%
    mutate(year = as.numeric(str_replace(term,'as.factor\\(did\\)','')) )%>%
    ggplot() +
    geom_hline(aes(yintercept = 0)) + 
    geom_pointrange(aes(x = year, y = estimate, ymin = estimate - 1.96*std.error, ymax = estimate + 1.96 * std.error)) +
    geom_smooth(aes(x = year, y = estimate), method = 'lm')


```

I don't think that that's a valid comparison, since you at least assume that the fished aren't a control. You could also just do a changepoint analysis?

Alternatively, let's think about this mechanistically. If you believe that the "targeted" classification is real, then by definition the MPA has no direct effect on the fished species. The real question then is whether there are trophic interactions that cause a problem. So, why not run a regression of unfished densities as a function of targeted abundance, controlling for other crap?

```{r, include=F, eval=F}

trophic_data <- density_data %>% 
  mutate(classcode = toupper(classcode)) %>% 
    left_join(life_history_data, by = 'classcode') %>% 
  left_join(temperature_data, by = c('site','side','year')) %>% 
  filter(is.na(targeted) == F) %>% 
  group_by(targeted, region, year) %>% 
  summarise(log_mean_biomass = log(mean(biomass, na.rm = T)),
                                mean_temp = mean(mean_temp, na.rm = T)) %>% 
  spread(targeted, log_mean_biomass) %>% 
  group_by(region) %>% 
  mutate(mb_lag1 = dplyr::lag(Targeted,1),
         mb_lag2 = dplyr::lag(Targeted, 2),
         mb_lag3 = dplyr::lag(Targeted, 3),
         mb_lag4 = dplyr::lag(Targeted, 4),
         temp_lag1 = dplyr::lag(mean_temp,1),
         temp_lag2 = dplyr::lag(mean_temp, 2),
         temp_lag3 = dplyr::lag(mean_temp, 3),
         temp_lag4 = dplyr::lag(mean_temp, 4)
         )

library(rstanarm)
trophic_reg <- stan_glmer(`Non-targeted` ~ Targeted + mb_lag1 + mb_lag2 + mb_lag3 + mb_lag4 + 
                    mean_temp + ( 1 |year) + temp_lag1 + temp_lag2 + temp_lag3 + temp_lag4, data = trophic_data )

# launch_shinystan(trophic_reg)

summary(trophic_reg)

post_interval <- trophic_reg %>% 
  posterior_interval()

post_vars <- row.names(post_interval)

post_interval <- post_interval %>% 
  as_data_frame() %>% 
  mutate(term = post_vars)

trophic_reg %>% 
  posterior_interval() %>% 
  as_data_frame(row.names = rownames(.))

reg_plot <- trophic_reg %>% 
  tidy() %>% 
  ggplot() + 
  geom_point(aes(term, estimate)) + 
  geom_errorbar(data = post_interval 
              , aes(term, ymin = `5%`,
                                                                 ymax = `95%`)) +
  coord_flip()

reg_plot

```

Interesting, some evidence of effect but it's messy, and depends a lot on model specification. 

## El Ni√±o


```{r}

a = read_html('https://www.esrl.noaa.gov/psd/enso/mei/table.html') %>% 
  html_node('body') %>% 
  html_text()


enso <- read_lines('https://www.esrl.noaa.gov/psd/enso/mei/table.html')

enso <- enso[str_detect(enso,'\t|(YEAR)')] %>% 
  write('enso.txt')

enso <-  read.csv('enso.txt', sep = '\t', header = F)

table_names <- enso$V1[1] %>% 
  as.character() %>% 
  str_split(boundary('word'), simplify = T) %>% 
  as.character() %>% 
  tolower()

enso <- enso %>% 
  slice(-1) %>% 
  as_data_frame()

colnames(enso) <-  table_names

enso <- enso %>% 
  gather('bimonth','enso',-year)

enso %>% 
  mutate(year = year %>% as.character() %>% as.numeric()) %>% 
  group_by(year) %>% 
  summarise(mean_enso = mean(enso)) %>% 
  ggplot(aes(year, mean_enso)) + 
  geom_point()

 enso <- read_table("http://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/Data/nino34.long.anom.data",
            na = c("-99.99", "99.99",'-99'), skip = 1, n_max = lubridate::year(Sys.time()) - 1870 + 1,
            col_names = c("year", 1:12)) %>%
 gather(month, enso, -year) %>%
 mutate(month = as.double(month))

```

## Get PDO

```{r}

 pdo <- read_table("https://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/Data/pdo.long.data",
            na = c("-99.99", "99.99",'-99'), skip = 1, n_max = lubridate::year(Sys.time()) - 1900,
            col_names = c("year", 1:12)) %>%
 gather(month, pdo, -year) %>%
 mutate(month = as.double(month),
        date = lubridate::ymd(paste(year,month,'01', sep = '-')))

pdo %>% 
  filter(year >= 2000) %>% 
  ggplot(aes(date,pdo)) +
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_point()


```



## Causal defense ideas

How can you defend the causal nature of the results?

Include a "lead" on the DiD term: This is turned on in the years leading up to the policy, and says that the policy is going to happen. SHould be insignificant if the model is right. Look back at mostly harmless and Olivier's note

How well does out-of-sample prediction help you with causality? At it's face it gives evidence that your model is doing a good job of describing the data. But does it imply help with causality? Suppose you had a model that said predict if it's raining outside based on umbrellas. It's out of sample prediction would be great, but that doesn't mean that opening umbrellas is going to cause rain. 

## Regression Exploration

Goal of this section is to start really digging into the regressions.

### Data Exploration

Let's stick with Jen's data for now, but cognizant that you need to look into reproducing and sensitivities to transformation assumptions

Pulling in Jen's density data, merging some useful temperature, site, life history, enso, and PDO data

```{r load data again}

density_data <- read_csv('../data/ci_reserve_data_final3 txt.csv') %>%
  magrittr::set_colnames(.,tolower(colnames(.))) %>% 
  gather('concat.name','value', grep('_',colnames(.)),convert = T) %>%
  mutate(data.type = gsub('\\_.*', '', concat.name),
         classcode = gsub('.*\\_','',concat.name)) %>%
  mutate(value = as.numeric(value)) %>%
  spread(data.type,value) %>%
  rename(site_side = site.side)

length_data <- read_csv('../data/UCSB_FISH raw thru 2013.csv') %>% 
    magrittr::set_colnames(.,tolower(colnames(.)))

temperature_data <- length_data %>% 
  group_by(site,side,year) %>% 
  summarise(mean_temp = mean(temp, na.rm = T))

life_history_data <- read_csv('../data/VRG Fish Life History in MPA_04_08_11_12 11-Mar-2014.csv') %>%
  rename(classcode = pisco_classcode) %>%
  mutate(classcode = tolower(classcode)) %>% 
  magrittr::set_colnames(.,tolower(colnames(.)))

life_names <- c('classcode',colnames(life_history_data)[!colnames(life_history_data) %in% colnames(density_data)])

life_history_data <- life_history_data[ , life_names]

site_data <- read_csv('../data/Final_Site_Table_UCSB.csv') %>%
    magrittr::set_colnames(.,tolower(colnames(.)))

site_names <- c('site','side',colnames(site_data)[!colnames(site_data) %in% colnames(density_data)])

site_data <- site_data[,site_names]

enso <- read_csv('../data/enso.csv') %>% 
  group_by(year) %>% 
  summarise(mean_enso = mean(enso, na.rm = T))

pdo <- read_csv('../data/pdo.csv') %>% 
group_by(year) %>% 
  summarise(mean_pdo = mean(pdo, na.rm = T))

comp_data <- density_data %>% 
  left_join(temperature_data, by = c('site','side','year')) %>% 
  left_join(life_history_data, by = 'classcode') %>% 
  left_join(site_data, by = c('site','side')) %>% 
  left_join(enso, by = 'year') %>% 
  left_join(pdo, by = 'year')
  
```

### Data Composition

Let's look at the distribution of sample size over time; where are your data coming from?

```{r}

comp_data %>% 
  group_by(year) %>% 
  summarise(num_obs = length(biomass)) %>% 
  ggplot(aes(year, num_obs)) + 
  geom_point()
  
```

```{r}
comp_data %>% 
  group_by(year,targeted) %>% 
  summarise(num_obs = length(biomass)) %>% 
  ggplot(aes(year, num_obs, fill = targeted)) + 
  geom_bar(stat = 'identity')


```

Huh, so a bunch of the observations are "unknown" on targeting status. Let's look into this. Beyond that though, the targeted and non-targeted are fairly well balanced. 

```{r}

huh <- comp_data %>% 
  filter(is.na(targeted))

sort(unique(huh$classcode))

```
Aha, these appear to be a a bunch of misc. critters and classification for understory cover. Aslo "young of the year". Probably best to filter these guys out. 

```{r}

comp_data %>% 
  filter(is.na(commonname) == F) %>% 
   group_by(year) %>% 
  summarise(num_obs = length(biomass)) %>% 
  ggplot(aes(year, num_obs)) + 
  geom_point()
  
```

```{r}

comp_data %>% 
  filter(is.na(commonname) == F) %>% 
  group_by(year,region) %>% 
  summarise(num_obs = length(biomass)) %>% 
  ggplot(aes(year, num_obs, fill = region)) + 
  geom_bar(stat = 'identity')

```
Huh, worth noting that a few of the islands only come in after 2003. What is this doing to your effect that you're basically brining in a bunch of new islands with different effects right after implementation of MPAs in 2003?

```{r}

comp_data %>% 
  filter(is.na(commonname) == F) %>% 
  group_by(year,broadtrophic) %>% 
  summarise(num_obs = length(biomass)) %>% 
  ggplot(aes(year, num_obs, fill = broadtrophic)) + 
  geom_bar(stat = 'identity')

```



### Effect Exploration

Let's look at the change in density as a functino of a variety of variables

```{r}

candidates <- c('biomass','year','site','side','classcode',
                'region','broadtrophic','reserve','campus','mpaareanm2','mean_temp','guild','trophicgroup','targeted',
                'mean_enso','mean_pdo','vbgf.k','vbgf.linf','max_length_fishbase')

reg_data <- comp_data[,candidates] %>% 
  filter(is.na(biomass) == 0, is.na(targeted) == F) %>% 
  mutate(log_density = log(biomass),
         factor_year = as.factor(year),
         targeted = as.numeric(targeted == 'Targeted'),
         post_mlpa = as.numeric(year >= 2003),
         reserve = as.numeric(reserve == 'IN'),
         did = as.factor(targeted * post_mlpa * year)
         )

```

```{r}

reg_data %>% 
  group_by(year,targeted) %>% 
  summarise(mean_density = mean(biomass, na.rm = T)) %>% 
  ggplot(aes(year,mean_density, color = factor(targeted))) + 
  geom_line()

```

```{r}

reg_data %>% 
  group_by(year,targeted,region) %>% 
  summarise(mean_density = mean(log(biomass + 1e-3), na.rm = T)) %>% 
  ggplot(aes(year,mean_density, color = factor(targeted))) + 
  geom_line() + 
  facet_wrap(~region) + 
  ylab('log mean density')

```

One troubling pattern, both ANA and SCI show that steep drop in densities from 2000-2003, which might really be skewing the effect of the MPAs (that early negative effect), though on the plus side it's both targetted and non-targeted 

Let's look at some other factors here

```{r}

reg_data %>% 
  group_by(broadtrophic) %>% 
  summarise(mean_density = mean(biomass, na.rm = T)) %>% 
  ggplot(aes(broadtrophic,mean_density, fill = broadtrophic)) + 
  geom_bar(stat = 'identity')

```
No surprises, higher density of herbivores

```{r}

reg_data %>% 
  group_by(region) %>% 
  ggplot(aes(region,biomass, fill = region)) + 
  geom_boxplot()

```

```{r}
reg_data %>% 
  select(biomass,year,mean_temp, mean_pdo, mean_enso) %>% 
  gather(metric,value, contains('mean')) %>% 
  group_by(year,metric) %>% 
  summarise(mean_biomass = log(mean(biomass, na.rm = T)), mean_value = mean(value, na.rm = T)) %>% 
  ggplot(aes(mean_value,mean_biomass, fill = metric)) + 
  geom_abline(aes(intercept = mean(mean_biomass, na.rm = T), slope = 0)) + 
  geom_point(shape = 21) + 
  facet_wrap(~metric, scales = 'free_x')
```

Interesting, definitely some noise in here, but seems to a a trend towards the "mean" of the environmental covariates. Good evidence for throwing a quadratic in here. 

```{r}

environmental_cor = reg_data %>% 
  filter(is.na(mean_temp) == F) %>% 
  select(mean_temp, mean_enso, mean_pdo, year) %>%
  cor() 

corrplot::corrplot(environmental_cor)

```
Quite a bit of correlation between some of the environmental variables, but you don't really care about estimating those precisely so not a huge deal. 

But, the year thing could be a bit messy, since you have such poor contrast before and after MPA, so might need to look at mean PDO as a problem variable. 

### DiD Structure

Now, what should the actual difference in difference look like?

The key thing you need is treatment on the treated, so that's being a fished species post 2003. 

If you include species specific fixed effects, then you don't have to include the general effect of being a fished species, since those will all come out in the wash (i.e. the fished effect is internalized in the species specific fixed effects, or alternatively, you say that the species level effect is a function of covariates, including being fished)

Similarly with post-mpa. If you don't include year fixed effects, then you need it. But, if you include year fixed effects, then the "post mpa" effect should be soaked up, and in fact perfectly colinear with, the year fixed effects. 

So, now, the real pain in the ass is the year terms. Which I think you figured out! 

The question then is how do you control for other things. 

You could just include the DiD term 

### Bare-bones regression

Let's start with a bare bones regression, using `rstanarm`, one hierarchichal, one not. You'll then compare your results to one where you code the likelihood yourself 

```{r, eval = F}

reg_vars <- c('log_density','factor_year', 'year','targeted', 'did', 'region' ,
'mean_enso','mean_pdo', 'mean_temp','classcode','site','side','post_mlpa','region',
'broadtrophic')

has_all <- function(x) any(is.na(x)) == F

pos_reg_data <- reg_data %>% 
  select(-mean_temp) %>% 
  left_join(conditions_data %>% select(site,side,year, contains('mean')), by = c('site','side','year')) %>% 
  filter(biomass >0, year >=2000) %>% 
  select_(.dots = as.list(reg_vars)) %>% 
  mutate(has_all_vars = apply(.,1,has_all)) %>% 
  filter(has_all_vars == T) %>% 
  map2_df(colnames(.), center_scale, omit_names = c('log_density','year','mean_enso','mean_pdo')) %>%
  mutate(did_dummy = 1 * targeted,
         did_year = paste('did',year, sep = '_')) %>% 
  spread(did_year,did_dummy, fill = 0) %>% 
  mutate(temp2 = mean_temp^2,
         pdo2 = mean_pdo^2,
         enso2 = mean_enso^2,
         site_side = paste(site, side, sep = '_'))

# pos_reg_data$did_2003[pos_reg_data$did_2003 == 1] <-  0.5

pos_reg_data %>% 
  group_by(factor_year,targeted) %>% 
  summarise(mb = mean(log_density, na.rm = T)) %>% 
  ggplot(aes(factor_year, mb, color = factor(targeted))) + 
  geom_point()


did_years <- paste('did',2000:2013, sep = '_')

did_year <- did_years[did_years!='did_2000']


simple_reg <-
as.formula(
paste0(
'log_density ~',
paste(did_year, collapse = '+'),
' + (1|year) + (1 + mean_temp + temp2 |classcode) + mean_enso +  mean_pdo + (1 | site_side) + (1 | site) + (1 | region) + targeted + post_mlpa'
)
)

# + vbgf.linf +
# vbgf.k +
# mean_enso + enso2 + mean_pdo + pdo2 + mean_temp  + temp2')

freq_flat_reg <- lme4::lmer(simple_reg, data = pos_reg_data)

flat_reg <- stan_glmer(simple_reg, data = pos_reg_data,chains = 1)


freq_did_plot <-  freq_flat_reg %>% 
  tidy() %>% 
  mutate(lower = estimate - 1.96 * std.error,
         upper = estimate + 1.96 * std.error) %>% 
  filter(str_detect(term,'did')) %>% 
  mutate(year = str_replace(term, 'did_','') %>% as.numeric()) %>% 
  ggplot() +
geom_hline(aes(yintercept = 0)) + 
  geom_vline(aes(xintercept = 2003), color = 'red', linetype = 2) +
  geom_pointrange(aes(year,estimate, ymax = upper, ymin = lower)) + 
  ylab('Estimated MLPA Effect') + 
  ggrepel::geom_text_repel(data = data_frame(x = 2003, y = 1), aes(x,y, label = 'MLPA Enacted'),nudge_x = 2) + 
  xlab('Year')
         
freq_did_plot

did_plot <- flat_reg %>% 
  as.data.frame() 
  
  did_plot <- did_plot[,str_detect(colnames(did_plot), 'did_')] %>% 
  gather(did,effect) %>% 
  mutate(year = str_replace(did,'did_','') %>% as.numeric()) %>% 
  group_by(year) %>% 
  summarise(mean_effect = mean(effect),
            top = sort(effect)[round(.975 * length(effect))],
            bottom = sort(effect)[round(.025 * length(effect))]) %>% 
  ggplot() + 
  geom_hline(aes(yintercept = 0)) + 
  geom_vline(aes(xintercept = 2003), color = 'red', linetype = 2) +
  geom_pointrange(aes(year,mean_effect, ymax = top, ymin = bottom)) + 
  ylab('Estimated MLPA Effect') + 
  ggrepel::geom_text_repel(data = data_frame(x = 2003, y = 1), aes(x,y, label = 'MLPA Enacted'),nudge_x = 2) + 
  xlab('Year')

did_plot
ggsave('new_mlpa_did.pdf',did_plot)

mtcars %>% 
  map_df(center_scale)

```

Now let's try and replicate with a custom STAN function

```{r, eval = F}

did_plot <- flat_reg %>% 
  as.data.frame() %>% 
  select(contains('classcode')) %>% 
  gather(classcode,effect) %>% 
  mutate(classcode = str_replace(classcode,'classcode',''))

stan_reg_data <- pos_reg_data %>% 
  mutate(year_marker = 1) %>% 
  mutate(year = paste('year',year, sep = '_')) %>% 
  spread(year,year_marker, fill = 0) %>% 
  mutate(classcode = paste('classcode',classcode, sep = '_'),
         class_marker = 1) %>% 
  spread(classcode,class_marker, fill = 0) %>% 
    mutate(site = paste('site',site, sep = '_'),
         site_marker = 1) %>% 
  spread(site,site_marker, fill = 0) %>% 
  select(log_density, contains('year_'), contains('did_'),
         contains('classcode_'), contains('site_')) %>% 
  mutate(constant = 1)

levels_to_drop <- c(min(pos_reg_data$year),sort(unique(pos_reg_data$classcode))[1],
                    sort(unique(pos_reg_data$site))[1])

stan_reg_data <- stan_reg_data %>% 
  select(-year_2000, -did_2003, -site_ANACAPA_ADMIRALS, -classcode_acor)
                    

y <- as.numeric(stan_reg_data$log_density)

x <- stan_reg_data %>% 
  select(-log_density) %>% 
  as.matrix()


stan_fit <- stan(
  file = '../scripts/ahnold_reg.stan',
  data = list(
    num_pars = dim(x)[2],
    num_obs = length(y),
    y = y,
    x = x
  ),
  chains = 4, 
  warmup = 1000,
  iter = 2000,
  cores = 4, 
  refresh = 100
)

did_terms <- which(str_detect(colnames(x),'did'))

did_plot <- stan_fit %>% 
  as.data.frame() %>% 
  select_(.dots = as.list(did_terms)) %>% 
  gather(did,effect) %>% 
  # mutate(year = str_replace(did,'did_','') %>% as.numeric()) %>% 
  group_by(did) %>% 
  summarise(mean_effect = mean(effect),
            top = sort(effect)[round(.975 * length(effect))],
            bottom = sort(effect)[round(.025 * length(effect))]) %>% 
  ggplot() + 
  geom_hline(aes(yintercept = 0)) + 
  # geom_vline(aes(xintercept = 2003), color = 'red', linetype = 2) +
  geom_pointrange(aes(1:length(mean_effect),mean_effect, ymax = top, ymin = bottom)) + 
  ylab('Estimated MLPA Effect') + 
  # ggrepel::geom_text_repel(data = data_frame(x = 2003, y = 1), aes(x,y, label = 'MLPA Enacted'),nudge_x = 2) + 
  xlab('Year')


```

OK! Things work. Moving over to `run_ahnold` for formal construction of this process

### Multi-level (hierarchichal) notes

OK, so I think I'm finally starting to get the hang of this. The confusion, from a regression point of view, is how mechanical do you have to be about the "multilevel part". 

Look at Gelman 12.15 and the code there. 

Your confusion has been, say you've got observations at the transect level, and you want to include species level covariates. In a fixed effects world this wouldn't work: you can't estiamtes species level fixed effects, along with things like vbk. that don't vary within a species. You can either include species fixed effects, or component things that define species. 

The way Gelman talks about this in multilevel modeling, is that you can instead model this in a hierarchical manner, where the coefficients of the species fixed effects are a function of things like vbk. This makes sense, but the question is the mechanics of this. My impression was that I would have to do this all manually, i.e. take the betas of the fixed effects, then write up a regression of those betas as a function of vbk etc. Makes sense but a total pain in the ass. 

Looking at Gelman page 266 though, it becomes clearer. It seems like mechanically you can do this just by converting the fixed effects to clustered random effects, and then including vbk as just another coefficient. 

e.g. `lmer(y ~ x + vbk + (1 | species))`


## Propensity Scores

```{r}

```

Add in broad bioregion effect

That could just be recovery inside and not spillover. Do you have to have spillover

They don't count the 

Look at Steve's BACI response to Ray's 

What happens to the regulaions outside in addition to the reserve. A simple model that looks at what would you expect 

Good data for the mainland 

Look into literature for priors 

I'm going to show that you can disentangle recruitment from lenghts

## 2017-03-15

### Expressing MLPA effect

Might be a good idea to show the significance statistically, but the effect through simulation. Simulate draws from the joint posterior with and without MLPA, show mean densities. Easier to understand, allows you to combine the net effect of hurdle component

## 2017-03-16

### Check in with Jen

* Check in with the idea of more local indicies of ENSO/PDO

* No big ENSO event till 2013

* Might need to explore removing painted greenling since they are bottom dwelling cryptic-ish. Tend to be counted more frequently when there wasn't much else. 

* Drop santa barbara island entirely (one off sample, 4 years)

* Let's think about parsing this by species. Fishing is very different across different species in here, what species are really driving the analysis? Are different species disproportionately driving the results 

* We have before and after data from the overflights, so could at least use that as a prior on the scale of redistribution. SAMSAP might be able to pick up the potential "blue paradox" side of things, as

* What if this is all just a recruitment signal 

* We could look at the SMURF data to get an idea of the recruitment trends 

* Something weird happened in 2008-2010 across all the CI data

* PISCO assumes that all species have the same probability of being seen anywhere in all the islands. Might be worth looking into what happens if you zero fill by data-base wide 

## 2017-21-03

Things are looking pretty strong. Basic Model diagnostics look solid for the most part, though the LMER approach is a bit skewed to the right. But no alarm bells. The biggest issue is really low effective samples sizes for several species, as well as collinearity between a few of the model parameters (load up the STAN run and launch shinystan to dig into this a bit). 

A few things you can do to try and deal with that. Re-run the model with only let's say the top 75% percentile by positive occurance species, to make sure that you're not getting too thrown by a bunch of really rare species. Can also try rerunning without painted greenling, since Jen seems to think those buggers might be a problem. I'm a little hesitant to do too much judgement based pruning of the data. Key things is does it really cause the results to break down. 

Jen also raised the issue of the zero filling. You did your zero filling by site. It seems that when Jen was going from raw lengths to densities, she was zero filling by region. So, a garibalidi should be missing from every transect in San Miguel. This seems a litle extreme to me, but you can play with this once you get the ability to move from lenghts to densities. I really want to hone in on any subjective decisions that might be driving the results, and this seems like a potentially big candidate
. 

Update: removing painted greenling has no real effect on results. Phew. 

Running now with only the top 50th percentile of species incorporated in the analysis, seeing what that does. 

One thought on the random vs. fixed effects thing: should species really be a random effect? Maybe you should only have varying slopes but not intercepts by species, so have fixed effects by species category, and then temperature and region effects by species.

Wooooo species effects don't matter either

You should go through and spell out why you think some things are random and some are fixed effects, drawing from the gelman framework, where the nature of the hierarchichal prior is really the defining characteristic 

## Sketching out Hypothesis Runs

What are the hypothetical states of the MPA world that you want to run over? One option is to specify a handful of boutique model runs: model 1, 2, 3 etc, each designed to evaluate a very specific set of circumstances. The other is to specify a set of key levels, and then monte carlo over combinations of those levers. 

Regardless, the key things seem to be

  1. Strength of density dependence
  2. Nature of density dependence
  3. initial b and f
  4. Larval movement
  5. Adult movement
  6. Age at maturity 
  7. Fleet reaction
  8. Species composition
  9. Scale of MPA relative to movement
  
The first 6 are relatively straightforward. Seems possibly easier to monte carlo and then pull out case study scenarios that you are interested in, since speed really isn't the issue here. 

You could also use that to easily create ensembles of species. 

So, for a given "run" you'd specify how many species you want to generate, and then project them and aggregate. So, you could specify one species if you want ot make it clear, or an assemblage of many different species. 

The risk with this approach is that you might just get a gigantic mess of results. But, you can easily pare this approach down to a concrete set if you want. I think this gets away from "cherry picking" scenarios that fit your story. You can present a giant array in trelliscope of outcomes, and then choose some to present in the paper. 


  
  1. Strength of density dependence
    * this is just steepness
  2. Nature of density dependence
    * You can use the babcock definitions that are already in there
  3. f
    * One option is just to say pick a random F
    * The other option is some kind of effort dynamics model, from open access to one way trip etc.
    * Seems excessive, the key thing is it overfished or not, and is it getting worse or not
    * Pick a random f, run it out
    * Stop the thing at some random point.... problem there is you can't be in "recovery"
  4. Larval movement
    * harder. Will take some thought to do this one right really. Key thing is do you want to deal with larvae or with recruits really. See how you do it in GASP  
  5. Adult movement
    * Easy enough, what proportion of adults move from each cell
  6. Age at maturity 
    * simple enough
  7. Fleet reaction
    * Dilute
    * Concentrate
  8. Species composition
  9. Scale of MPA relative to movement
    * Make the MPAs about the pattern of CI

For now, let's take the life history straight out of the data that are actually used. So, grab the species that make it through the filter and project those. So, each run will take the life history data and run with it with a random assortment of traits. 

Let's focus on keeping it simple stupid. Take the species, project forward with a random F and a random stop point, a steepness drawn from the family distribution for that species, a random density denepence form... let's spell it out


1. Draw a species from the database
2. Fill in missing non-variale life history characteristics (e.g. linf)
3. Randomly assign characteristics of interest (density dependence form, movement, etc.)
4. Create completed fish object
5. Create a fleet object
6. Fill in the fleet object with things like F, or fleet model dynamics etc. 
7. Pass a completed fish and fleet object to `sim_fishery`
8. Store results
9. Repeat runs a whole bunch of times. 
10. Aggregate as desired

# 4/13/17

As long as the errors in your dependent variable are mean 0, then there's no problem. If they are not, or if the errors are a function of your dependent variable, then you do have  problem

Look in to stratification from that book of Kyles



## Check in with COdy

Make mortality/growth rates a function of biomass

Check on lit on this 

Make plot of length on x and year on y, so you get a bunch of stacked shifts in cohorts

Maybe add in a ricker function, though might need to double check on the steepness values. You might be moving things over the to right hand side of 

# Examine OST catch data


```{r}



ost_region_catch <- read_csv('../data/SouthCoastHumanActivitiesCommercialFisheriesEconomicandSpatialData1992to2012/sc_comm_fisheries_region_landings_data_1992_2012.csv') %>% 
  set_names(colnames(.) %>% tolower())

ost_region_catch %>% 
  mutate(pounds = str_replace(pounds,',','') %>% as.numeric(),
         revenue = str_replace_all(revenue,"\\$|,",''),
         price = str_replace(`average price`,'\\$','')) %>% 
  group_by(year,fishery) %>% 
  summarise(catch_lbs = sum(pounds, na.rm = T)) %>% 
  ggplot(aes(year,catch_lbs, color = fishery)) + 
  geom_vline(aes(xintercept = 2003), linetype = 2, color = 'red') +
  geom_line()

```

Interesting, stablish throughout the SC region, though that's a pretty damn big area

```{r}

ost_port_catch <- read_csv('../data/SouthCoastHumanActivitiesCommercialFisheriesEconomicandSpatialData1992to2012/sc_comm_fisheries_port_landings_data_1992_2012.csv') %>% 
  set_names(colnames(.) %>% tolower())

ost_port_catch %>% 
  filter(port %in% c("Port Hueneme/Oxnard", "Santa Barbara","Ventura")) %>% 
  mutate(pounds = str_replace(pounds,',','') %>% as.numeric(),
         revenue = str_replace_all(revenue,"\\$|,",''),
         price = str_replace(`average price`,'\\$','')) %>% 
  group_by(year,fishery) %>% 
  summarise(catch_lbs = sum(pounds, na.rm = T)) %>% 
  ggplot(aes(year,catch_lbs, color = fishery)) + 
  geom_vline(aes(xintercept = 2003), linetype = 2, color = 'red') +
  geom_line(show.legend = F) + 
  facet_wrap(~fishery, scales = 'free_y') + 
  theme(axis.text.y =  element_blank(),
        axis.text.x = element_blank())


```


# 4/14/17

I'm honestly close to out of ideas. These catch dat are pretty worring to be honest. If you buy these data as being representative of the fished species out in the channel islands, then this suggests a decreasin gcatch trend in nearshore finfish catches, not stable or increasing. That really makes it hard to explain the MPA mediated "decrease" in abundance. 

Possible explanations that remain. 

1. Some form of density dependent mortality (or ricker like dynamics)
  * Can model this

2. Catches at the islands themselves have been stable or gone up
  * Can try and get some more regional data to check on this
  
3. Ocam's razor: There was just a dramatic enough shift in sampling regimes when the MPAs went in place that pre-and-post are just not comparable in any way shape of form

4. Let's write up the current results and state of the world this weekend/next week and send that to committee for comments

# Scraping CDFW Data


```{r}

library(tidyverse)
library(lubridate)
library(stringr)
library(tabulizer)
library(tabulizerjars)

a = tabulizer::extract_tables("../data/cdfw-data/landings00_table12.pdf")

x <- a[[1]]

x <- as_tibble(x)

process_cdfw <- function(x){
  
  x <- as_tibble(x)
  
  x <- slice(x, -(1:2))

    numfoo <- function(z){
    
    z <- str_replace_all(z, '\\.|\\,','')
    
    if (any(is.na(as.numeric(z)) == F)){
      
      z = as.numeric(z)
    } else{
      z = z
    }
    
    }
    
    browser()
    x <- map_df(x, ~numfoo)
  
}

 b =  map_df()

map(a, process_cdfw)

```

# 4/20/16

So you've got the california commercial landings now. You can link them up to most of the main species in your database. So, what do you do with this. Was a lot more work than you though it was going to be, so seems like something useful should come from it. 

At it's face value, catches do a few different things across the islands, and none of them really look like constant catch or constant effort. Maybe open access?

If the goal is to examine realism, the best thing you could do would be to link these catches directly to the operating model. So instead of trying to estimate catch trajectories, take the data. 

Now, one of the inputs would be depletion in 2000, and then subtract the catches out from there, add in the MPA in 2003, and see what happens. 

Another option, pair these with the length data, and actually fit Catch-MSY/CC-SRA, etc to the species of interest. Use that to get estimates of status in 2000, as well as K, and then project forward with those catches + mpa... doesn't really work though since the MPA should in theory be in the estimation model. But might be a path to explore later. 

So for now, you'd need to choose an initial depletion and an r0. Tuning an initial depletion is just an f, that's easy. But now how do you deal with the catches part. Trickier. One option would be to tune r0 such that that eq catches in 1999 are about equal to the catches in 2000. Basically says that the catches in the prior year were probably similar for a given F and depletion 

```{r}
library(spasm)


tune_f <- function(params, target_depletion, target_catch = 100, alpha = 0.5){

fish <- create_fish(linf = 100, m = .2, r0 = exp(params[1]))

manager <- create_manager(year_mpa = 999)

unfished <- sim_fishery(fish = fish, fleet = create_fleet(initial_effort = 0, fish = fish), manager = manager, num_patches = 10)

ssb0 <- unfished %>% 
  filter(year == max(year)) %>% 
  summarise(ssb = sum(ssb)) %>% {
    .$ssb
  }

fished <- sim_fishery(fish = fish, fleet = create_fleet(initial_effort = exp(params[2]), fish = fish), manager = manager, num_patches = 10)

ssb <- fished %>% 
  filter(year == max(year)) %>% 
  summarise(ssb = sum(ssb)) %>% {
    .$ssb
  }

fish_caught <- fished %>% 
  filter(year == max(year)) %>% 
    summarise(caught = sum(biomass_caught)) %>% {
    .$caught
    }


depletion <- ssb/ssb0

ss <- alpha * (log(depletion) - log(target_depletion))^2 + (1 - alpha) *(log(fish_caught) - log(target_catch))^2

return(ss)
}


a = nlminb(c(log(100), log(10000)), tune_f, target_depletion = 0.2,target_catch = 200, alpha = 0.5)

fish <- create_fish(linf = 100, m = .2, r0 = exp(a$par[1]))

fleet <- create_fleet(initial_effort = exp(a$par[2]), fish = fish, 
                      fleet_model = 'constant-effort')

fished <- sim_fishery(fish = fish, fleet = fleet, manager = manager, num_patches = 10)

catches <- fished %>% 
  group_by(year) %>% 
  summarise(catch = sum(biomass_caught))


fish <- create_fish(linf = 100, m = .2, r0 = exp(a$par[1]))

my_catches <- c(catches$catch, runif(10,20,100))

fleet <- create_fleet(catches = my_catches, fleet_model = 'supplied-catch', fish = fish)

burn_years <- 20
manually_fished <-
  sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = manager,
  num_patches = 10,
  sim_years = burn_years + length(my_catches),
  burn_year = burn_years
  )
  
manual_catches <- manually_fished %>% 
  group_by(year) %>% 
  summarise(catch = sum(biomass_caught))

plot(manual_catches$catch, my_catches)

mpa_fished <-
  sim_fishery(
  fish = fish,
  fleet = fleet,
  manager = create_manager(year_mpa = 12),
  num_patches = 10,
  sim_years = burn_years + length(my_catches),
  burn_year = burn_years
  )
  
mpa_fished %>% 
  group_by(year) %>% 
  summarise(mpa = mean(mpa)) %>% 
  ggplot(aes(year,mpa)) + 
  geom_point()

manual_catches<- mpa_fished %>% 
  group_by(year) %>% 
  summarise(catch = sum(biomass_caught))

plot(manual_catches$catch, my_catches)

```

Shit works!!!

So, the order of operations

1. Get life history for a species

2. Get catches for that species from CDFW

3. Pick a target depletion

4. Pick and MPA size and year (mimick CI)

5. Find f,r0, and catches per above

6. Tack CDFW catches onto the "historic" catches 

7. Project forward with MPA. 

Moveing over to a dedicated script 


# 4/23/17

You need to do some early stage data exploration again. 

Follow some of the steps from the zuur paper, specifically diving back into the multi-collinearity problem. 

You need to figure out the sample size and balance issues

Basically, what I'd like to check is that the relationship isn't just due to the massive increase in sample size at the start of the MPAs

I also really want to re-run things building up the densities from scratch. But you also need to focus on getting some clear metrics and deliverables based on QAQC so you can move on and traige more quickly whether there's anything real here. 

Tomorrow:

1. Explore colinearity problems with the data

2. Try and recompile the density data from scratch again. 


# Review from Kyle's Class

* Idea: repeat Jenn's analysis, but compare the rate of change inside and outside for fished species across gradients of larval/adult movement. THe idea would be that the non-likely to be affected by MPAs species are a better control, and so you'd use those for the control trend pre mpa for the fished species. 

So, then you use the fished species outside the reserve as the control 

# 2017-06-14 - Triage update

Your goal in triaging has been to understand what drives the current main results, which seem to suggest that MPA caused overal decrease but an increase inside the MPA. 

The problem is that I can't really seem to explain that observation. 

So, what's causing that?

Simulating the shit out of it doesn't seem to do the trick. Almost any combination of stock recruitment relationships, movement rates, fleet dynamics, etc. don't produce that observed behavioral pattern. 

Adding in the observed catch histories as well doesn't do it. 

Isolating the data doesn't seem to do it. 

Same pattern if you include only mature fish

Same pattern if you degrade the post 2003 data

Same pattern if you use only consistently sampled sites and species

Same pattern if you recalculate the densities based on the raw length data

I'm stumped at this point. The model says all else being equal, it's worse to be a fished species right after the MLPA than right before. That's a cool story, but really hard to justify that. 


# 2017-07 Update

You need to make a decision on this in the very near future. Or do you? If coaseian tuna is going to sub in here if this one doesn't get written, and Coaseian tuna is already in pretty good shape, then maybe the best thing to do is focus on skynet and scrooge, and shelve this for a bit as either a) close to being done or b) easily dropped and subbed in by Coaseian tuna. 

What you do need then though is a sense of where you are. Go through here and clean up the repo a bit and try and make sense of all ths various places that things live...

What you need is a cohesive report of all lf your efforts to explain the observed patterns estimated by the model. 

Schedule a meeting with everyone to present your results and see what we think. 

mlpa-effects-updat.Rmd seems to be the place to do that

run-cdfw-mpa-sims.R and run-mpa-sims seem to be the two places where you've this this thing documented

This would also be a good project to set up on the cloud, just cranking away on tons and tons of MPA simualtions to see when and where MPAs cause improvements

# 2017-07-19 Check in with Steve

The problem: I can't find a viable explanation for the sudden drop in predicted densities the model predicts in fished species. 

In other words, the model broadly suggests that the parallel trends assumption was valid, and that the parallel trends were stable pre MLPA (interaction effects between fished species and year was zero pre-2003). 

Suddenly, here comes the MLPA in 2003... and the coefficients take a nose dive, and then slowly recover. In other words, the predicted density 

Suddenly in 2003, we see this sudden downtick, meaning that, being a fished species post-mlpa means that all else being equal, densities for fished species went down, relative to being an unfished species post-MLPA. 

Let's make sure that you are interpreting your model right and this isn't just a case of stupid variable interpretation. 

In theory, the DiD estimator should be fished, post-MLPA, and the interaction. 

So, if it's a fished species post MLPA, that means that relative to being an unfished speices (0)
plus the general effect of post MLPA (1) 


log_density ~ did_2000 + did_2001 + did_2003 + did_2004 + did_2005 + 
    did_2006 + did_2007 + did_2008 + did_2009 + did_2010 + did_2011 + 
    did_2012 + did_2013 + (1 | year) + (1 + mean_temp + temp2 + 
    region | classcode) + mean_pdo + (1 | site) + targeted
    
That's the regression chosen by AIC selection. 

The effect is pretty damn stable to model selection. 

Besides variables included then, I've tried

1. Only using consistently sampled sites and species
2. Removing species one-by-one and refitting the model
3. Degrading the quality of the post 2003 data (same sample size as before)
4. Recalculating the densities from the length data
5. Only counting densities of mature fish

All of those produce the same trends, which makes one think, OK, something good

The problem is, I then tried to simulation test things to see what state of the world would cause that, and I can't get it to work. 

Tried it with all kinds of life history and fleet dynamics, and biomass with MPA > biomass without MPA 

Scraped CDFW catch data, used the actual catch histories for the species involved, and same deal. 

So, what the hell explains this downtick, unless I'm just observing something else. 

Is the problem maybe just a lack of a damn parallel trends assumption?

Under parallel trends, the difference in the means should be more or less constant: different intercepts, same slope. 

So, in theory, if you took take the mean difference in employment in miami and atlanta, subtracted them, you'd get say more employment in atlanta, but following the same southern trend. You then look at miami post intervention, and that trend has broken, and the amount by which it has broken is the causal effect.

So in your case, there's an overall year effect, and an overall targeted effect, and the regression coefficient is then being targeted post MLPA. 

And of course you're overthinking these. For all the fanciness, what the model seems to be picking up is that from 2003 to 2003, densities of fished species droped wayyyy more than densities of unfished species, and then started to climb up a bit. 

The parallel trends in your model come in in the year effect. So, if you already control for being a fished species, and the year being 2004, and then ask the additional effect of being a fished species in 2004, that's essentialyl saying how much different than the unfished species in 2004 is the fished species in 2004. If there's no difference, the effect should be zero, since it should just be the density of the unfished species, plus the intercept of the fished species. 

The fundamental problem then is the massive drop in densities of fished species from 2002 to 2003, which drops a lot quicker than the densities of unfished specise


** Need to add section to chaper discussing current regulations in the evaluated MPAs and where they fit in the "no take" literature**


# 2017-09-16 HOLY SHIT THIS MIGHT BE THE SOLUTION

YOU'VE BEEN STRUGGLING LIKE CRAZY WITH HOW TO DEAL WITH THE ZEROS IN THE DID

Also, it's been really unclear what's causing the dip, and you keep attributing it to this "other" things. Well instead of measuring densities, try and measure abundance as standardized densities, and then fit the DiD model to that, now controlling for all the things that affect abundance soaked up in year (el nino, and of course, MPAs!)

One option would be to actually standardize each species CPUE, and then fit the DiD model to the standardized index of abundance, instead of to the raw densities??? Because what you really want to do is measure the effect of the MLPA on abundance, not just observed densities. and right now you control for a bunch of other stuff but it‚Äôs a bit confusing. 

So what if you said get the standardized index of abundance in the above manner. Then, estimate the DiD on those terms, controlling for things that were omitted from the abundance index, e.g. el nino, water temperature, etc. that‚Äôs pretty fucking good. 

You'll see if this works and makes a broad difference deterministically, and if it does, then you'll move to STAN for a full accounting of the uncertainty.  

# Check in with Ray

Catch was more or less reduced in proportion to the area constant effort and then that scorched earth outside

System wide effect you wouldn't expect much inside and and more of an effect outside

Would expect 

Arlinghouse's lab 

How much of the total catch is represented 

You should also look into weighting by area of the sites/region, instead of weighting every observation equally 


# Rebuild of run-ahnold

There are so many damn things that you're trying to do at this point that it's getting confusing as all hell to keep track of what you've tried, if you've tried it with the same data, and what the results are. Let's lay out a strategy to create one cohesive run. 

## Load run

The usual stuff here, but focus on running lots of options at onces, rather than setting run-specific options. 

## load and process data

load in the raw data, aggregate and join, etc. 

## filter the raw data 

## prepare model runs

For each set of input data (e.g. consistent only, no sebastes, etc, mpa only), for each set of treatment effects (2002, 2003, sliding, age at maturity binned), run the model, where "the model" means estimate fit the "only on the positive", then the double-delta method. But I don't really want to put a whole shit ton of work into automating the world if you're not going to use the world. So what are the key questions that you want to answer, and is it worth going bayesian on it. 

You have to break this into two very annoying steps. One is to build up the abundance indicies and the raw data on different data subsets. Once you have the different abundance models, you then have to pass that to the DiD estimator. 

### data subsets (for both jen and calculated)

  - All sites

  - consistently observed only

  - mpa only


### population structure

1. The simple model (no zeros). That one is pretty cut and dry

2. one population

3. regional populations

4. mpa and non-mpa populations

god damn it. This all has to be done in the 

### treatment effects

- 2002

- 2003

- sliding

- growth scaled 

### Variables

- kitchen sink

- pared down

```{r}
sample_frame <- tribble(~dep_var,
                        ~ind_vars,
                        ~ treatment,
                        ~data,
                        ~pop_structure,
                        ~model_type)
```


# Hypothesis testing

The end result of the above process then will be a model of MPA effects for a wide range of data subsets and treatments. 

From there, the question is what is the effect. 

The simplest thing to do is what you already have done: comparing the null effect as zero. 

However, as Chris pointed out, is that really the right null? What you almost want here is a structural approach, where you accept or reject more detailed hypotheses. E.g., can you reject a 640% increase in system wide biomass. While you might not be able to reject 0, you can probably reject that. 

So, you can generate your huge list of scenarios, and one by one accept or reject them, based on the estiamted trend in abundance with and without MPAs, relative to the year 2000 or something. 

That can then be refied to pull in specific MLPA things. 

I really really like that. Let's get to work. 

# 2017-10-10 - lessons learned from alternative abundance trends

So what have you learned here? The exercise was to run a bunch of different ways to generate an index of abundance for each of the species in the database. 

This included data filtering (all data, inside MPA only, consistent sites only), as well as population structure (one population vs island meta populations vs MPA and non MPA), and of course your own data vs jenns processed data

The good news is broad trends are very insensitive to basically any of these choices for many species. Given how poorly fit the model is in many instances, seems to make more sense then to try and reliable estimates for the whole population rather than diluting the sample size

One interesting feature, looking at the MPA data only seems to remove that huge downtick that the models realllllly seem to like to do in the early years from the standardized data. Probably worth checking to make sure that you're not butchering the standardization, though that would seem inlikely. 

While the choice of what data and what pop don't matter all that much. However, to standardize or not to standardize makes a pretty damn huge difference. copper rockfish is a pretty great example of this. where you've got either a steady increase predicted by the raw data or a stead decrease from the standardized index. 

the standardization really seems to like extreme spikey behavior, which I'm suspecting has something to do with the hurdle part of the model. gopher rockfish would be a good diagnostic case for looking into that. Brown rockfish is another pretty good example. 



Really spiky as well, would be good to look into hierarchichal or GAM approaches here to see if that helps. 

Pile perch is another good one, with very spikey abundance estimates. Should just browser in on a few of these and test out a few different approaches, brown rocksih is a really good one though. Rubberlip as well

brown rockfish
gopher rockfish
rubberlip seaperch
pile perch
white seaperch (crazy spikey both in raw data and in standardized)

# 2017-10-12 fine tuning abundance indices

OK, so part of the problem before was that some species were poorly observed over time, i.e. not observed at all in a given year. So, the observation model was failing in those years, producing a 0% probability of detection, seriously throwing off the model. 

So, as part of the filtering you're now estimating the model only on species that have been observed every year, the idea being you don't want to skew the analysis with species that are so rare that they are not observed at all in pre-or-post years. 

That obviously skews away from say san miguel species, but seems like the cleanest form of analysis that you can: for species that you observed before and after, what happened? 

Looking at the results, still seeing some annoying spikes in effects . Copper rockfish seems like an interesitng one to take a look at and see what's causing the huge spike in abundances in the earler years (in the standardized data). Brown rockfish as well, and rock wrasse. 

Let's take a look at those species and see how the probability estimations are going. 

If that looks good, then revisit a GLMER/GAM approach to this to see if that helps smooth out the spikes in the year terms. 

Looking then at a species with that percipitous deline in the early years, we see something like brown rockfish. Now before that sharp ass decline was often a thing with the probability of detection. In that speices, not the case, and in fact the prob seen if inflating abundance in the future. Pretty crazy though that it goes from over 300 to nearly zero in the linear prediction though. 

part of that does seem like it could be the site thing, since you don't see that drop when it's consistent sites only. 

# 2017-10-19

OK you're getting somewhere. There are definitely some problems with trying to do fine-scale population dynamics in terms of model fitting: lots of rank deficient fits, even if you drop out the observer effects. 

One potential thing to try. What if you fit all the species at the same time? i.e have year:species interactions, and then all the other terms that you need. The idea there being detection/observer bias is consistent across species, and the only difference in trends over time then is population. Let's mess around with that and see if you get better estimation of parameters. 

If that doesn't work... Then I think you need to do a run filtering down to only non-rank deficient fits and see that that leaves you with in terms of sample size. 

Once you test those two things, you're on to fitting the DiD estimator. 

This should be back in familiar territory. It doesn't really make any sense to change the implementation year itself, since in theory that should just be picked up as a "zero" effect for that year. 

But, you could think about treatment as something like "generation time protected", or "growth years" protected. This gets a bit tricky to think about how to do in a defensible way, but definitely worth exploring. 

So through your filtering it seems that you're losing years again. Need to go back and make sure that you have a filter for only species that have been seen every year 2000-2013, just as your base case of the most reliable results possible tnc

# 2017-10-23 v2.0 up and running

v2.0 has been benchmarked, meaning that you now have DiD estimators fit to a whole slew of different data inputs. Woot!

So what now?

I'd like to test out a Bayesian state-space approach, on the simplest case possible (one population), to just get a sense for how different the results might be. That way, you have something like a bookend of all the possible outcomes for this analysis and you don't leave any "well if I only tried this" things out there for reviewers. 

Let's doa  bit of testing that idea, just to make sure you know how to use STAN correctly. GOod excuse to get better at it anyway. 


# 2017-10-30 

Might need to think about doing things at the monthly level and controlling for month, there is a shift in the months sampled and there is some evidence that it leads to to slightly different densities, see below if you include month in the length_to_density data

```{r, eval = F}
a <- length_data %>% 
  mutate(date = lubridate::ymd(paste(year, month, day, sep = '/')),
         mmonth = lubridate::month(date),
         yyear = lubridate::year(date)) %>% 
  ggplot(aes(x = mmonth, y = factor(year))) + 
  geom_density_ridges()

a %>% 
  mutate(year = lubridate::year(date),
         month= lubridate::month(date)) %>% 
  group_by(year,month) %>% 
  summarise(ns = length(classcode)) %>% 
  ggplot(aes(month, ns, fill = factor(year))) + 
  geom_col(position = 'dodge')

length_to_density_data %>% 
  ggplot(aes(month, total_biomass_g)) + 
  geom_point()

a = lm(
  log(total_biomass_g) ~ factor(month) + factor(year) + classcode,
  data = length_to_density_data %>% filter(total_biomass_g > 0))
  

v <- broom::tidy(a) %>% 
  filter(str_detect(term, 'month'))


v %>% 
  ggplot(aes(term, estimate, fill = p.value<= 0.05)) + 
  geom_col()

```


# Stanifying DiD model

So after some thought, it seems like STAN is a much better fit for running the DiD part of the model, allowing you to much more efficiently make the regression hierarchichal in nature. The question then is how to actually do that. The point is not to poke endlessly at various model configurations but to come up with some distinct hypotheses to take a look at. 

The clearest ones that you already have are what the "DiD" estimator actually is: time, generations protected, recruits protected, etc. 

So, it seems like one class of model to run is an absolute bare-bones DiD estimator with these things in there. treatment, control, interaction. That model should in theory control for time-invariant differences. Take two cities that want to see the affect of a policy on health outcomes, one treated one untreated. If you just look at the pre-post for the treated city, any observed changes might be do to broader regional trends. If you look at the post difference between the treated and untreated the difference might be a characteristic of the city itself (wealthier in the first place). So, you look at the change in the difference between the two cities, and that nocks out both the trend and the inherent differences. So, takes care of omittied variable bias such as "city wealth", assuming that city wealth doesn't change over time. i.e. it doesn't help you if the control city gets magically richer after the control period. So what's the point of including things specifically? Relaxes that specific assumption for that one trait, but you'll never control for everything, so knocks out things that you can't control for under the assumption that they stay constant. So, let's think about something like ENSO. ENSO affects densities, as does the MPA. If you just compared densities pre and post MPA,  the affect of ENSO might be screwing things up. So, if we take the assumption that ENSO affects fished and unfished species equally, then the unfished species serves as a trajectory control for where the fished species would have been given ENSO but without the MPAs. So what's the logic of explicitly controlling for ENSO? Looking back at mostly harmless page 236 provides an example. They just do pre and post, and also include adult employment. So, under the assumption of the DiD without including adult employment, both treatment and control cities have the same trend in adult employment, and so it's knocked out. Suppose though that the have marginally different trends in adult employment, including it in the model now accounts for that. Now, there's a problem with including covariates that also respond to the treatment, which clearly isn't a problem for environmental drivers.But, let's ONE MORE DAMN TIME think through the logical of regional-level drivers like ENSO. The goal is parallel trends in species. If you had controls in different places, ENSO makes sense. But, since ENSO is the same every year for every species in the database, then it doens't actually do anything for you here that the fixed effects don't do. In other words, if your goal is to compare the difference in each year between the fished and unfished species, and the fished and unfished species both internalize ENSO the same, then there should be no usefulness of explicitly incorporating ENSO (and in fact it won't work in a fixed effects framework). 

So, including annual level drivers is only useful for two reasons. The first is from a hierarchical modeling framework, in that it helps you obtain better estimates of the year effects. This is similar to the county-wide radon effect thing from Gelman. Assuming you have repeated measurements per county, then the radon essentially serves as a prior on the county level effect (higher radon = higher health effects), so helps pull the estimate of a specific county up. So, in the same way here. If on average high ENSO makes density higher, then including ENSO in a hierarchical model would help improve estimates of the year effects in a given year. 

Or, in your case, it helps improve estimates of the fished and unfished year terms. 

The other reason that it's potentially useful is if ENSO/other environmental drivers have different affects on different species. This is somewhat analogous to the region-specific trends from mostly harmless 238. 

That's more similar to how you're doing things now, where you allow the effects of annual environmental drivers to vary by species. 

What about including targeted? If you include species specific intercepts, then targeted should just work to help refine those intercepts a little bit. Those are the big drivers that you need to think about then....

So, let's write out hypotheses

## Perfect DiD

This is a useful one to run just by comparison. So in this one, you assume that simple DiD is perfectly satisfied, and so you just fit a model of 

targeted + post_mpa + targeted:post_mpa (double check that this works the same as the manual interaction)

This assumes that environment affects the fished and unfished species the same and there are no divergent trends in other variables for these species

## Effects evolve

So now, rather than assuming a pre and post, you want to allow the DiD estimator to evolve over time. So now it's

targeted + (1|factor_year:targeted)

This is mostly equivalent to the above, but now instead of just controlling for treated not treated pre and post, you control for treated-not-treated over time. So, in a perfect world, suppose that the evolution of the common trend is just a little lumpy, takes time to evolve, etc. If the policy effect was 0 for the first few years then huge, the net measured effect would be small, but you want to see that deviation. So, by estimating fixed effects for each year by targeted/not targeted, the year:targeted interaction should be the divergence between the fished and unfished species. So what we'd like to see is that given an intercept for targeted species, we get significant year effects for the unfished species and intercept, but zero effect for the targeted species in the early years since those terms should be unfished + intercept. But, in the later years, the fished term should start to be important, and so you'll see a divergence. 

Now the problem with that. AHA, no you're not doing that right. At the moment, you're not really doing the DiD part of all this if you do it as factor_year:targeted. The problem with how you have it now, dumbass, is that the unfished years are always off for the fished species and vice versa: this is basically the same way you were getting regional population trends. THAT'S NOT WHAT YOU WANT. what you want is year fixed effects that are turn on no matter what, and then an added bonus effect if you're a fished species in that year. So, how do you do that. 

(1 + targeted|factor_year)

That gives a varying intercept for every year, and a varying "slope" for targeted in each factor year. Think about it like it was the environmental stuff. You were saying that every species has a intercept and a different effect of ENSO. This is saying every year has an intercept and a different effect of targeted. So, being a targeted species in year 2000 should have no affect beyong the mean abundance trend and the targeted effect, but should take on some additional effect over time. 

## Environment fine tunes year intercept

environment goes in as fixed effects

## Environment affects different species differently

environment goes in as random effects of classcode


## generation time did

So how do you actually want to structure the generation DiD, given what you have here....

The simplest scenario is once again 

targeted + at_least_one_generation_protected +  targeted:at_least_one_generation_protected

What does that look like if you try and compare it over time....

The counterfactual is an unfished species that has had N generations protected... So to get that you could do


(1 + targeted)|generations_protected

In that there is some baseline effect across all species of a given number of generations protected... but that affect may change for targeted species? The slight problem with this is that I'm not sure the algebra works out since that implies different "time" units... I guess the assumption is that the population trends operate at the time scale of generations, not years, in order to get to parallel trends, and so this is really a sort of transformation. 

 Before you go crazy here, I really want to test whether this idea works. 
 
 # 2016-12-5 Post validation update
 
 OK, what you should have done a long time ago, went and created a markdown called test-did.Rmd. In there, I'm testing the ability of different estimators to nail the effect. As you suspected, you were kind of butchering the DiD estimator for a bit now. The key now seems to be only having random effects on species (e.g. ENSO), and making sure that you include a fixed effect for targeted. That zeros things our properly, otherwise you get better estimates but it's not the "causal" effect. 
 
 So, running that now. THat clears up the "where should the environment go" think so puts you in a world of simple vs + environment effects, and then the generational DiD idea. 
 
 Definitely need to test that one out a bit to make sure it actually works
 
# Reformating STAN model

Going to have a talk with Cole, but he already got us thinking in some good directions. First, max tree depth. Default is 10, meaning that ~2^10 is the max number of leapfrog steps, aka a lot. The reason it slows down so much in your model must mean that it's hitting this max tree depth a lot, meaing, per the stan-reference 2.17.0 on page 320, that you probably ust have an improper posterior, that you should try and fix! 

Booya. 


# Retooling stan model

Part of the problem seems to be that the model is having a reaaaaaaaly hard time mixing for a lot of the species/region effects. I suspect this has to do with the extremely low sample size for some species in some regions. One solution might be to to do region effects clustered by broad traits, either by temperature, mean longitude, or by PCA... 


```{r}

seen_data %>% 
  group_by(classcode, region) %>% 
  summarise(obs = length(log_density)) %>% 
  arrange(obs)

data %>% 
  group_by(classcode, commonname) %>% 
  summarise(temp = unique(temperature)) %>% 
  ungroup() %>% 
  mutate(commonname = forcats::fct_reorder(commonname, temp)) %>% 
  ggplot(aes(commonname, temp)) + 
  geom_col() + 
  coord_flip()

```

You could certainly bin things by temperature. What about a PCA by region.... or actually then something like k-means clustering, but let's look at the PCA first to see how it looks


Let's see the PCA first 

```{r}

sightings <- length_to_density_data %>% 
  filter(is.na(total_biomass_g) == F & total_biomass_g > 0,
         classcode %in% abundance_models$classcode) %>% 
  select(classcode, commonname, site, side) %>% 
  left_join(site_coords, by = c('site','side')) %>% 
  filter(is.na(latitude) == F & is.na(longitude) == F) %>% 
  group_by(classcode, commonname) %>% 
  summarise(mean_lat = mean(latitude),
            mean_long = mean(longitude),
            min_lat = min(latitude),
            max_lat = max(latitude),
            min_long = min(longitude),
            max_long = max(longitude)) %>% 
  ungroup()

num_clusters <- data_frame(clusters = 1:20) %>% 
  mutate(within_ss = map_dbl(clusters, ~sum(kmeans(sightings %>% select(contains('_')),centers = .x, nstart = 25, iter.max = 1000)$withinss)))

num_clusters %>% 
  ggplot(aes(clusters, within_ss)) + 
  geom_point() + 
  geom_line()

sample_cluster <- kmeans(sightings %>% select(contains('_')),centers = 5, nstart = 25, iter.max = 1000)

sightings <- sightings %>% 
  mutate(cluster = sample_cluster$cluster)


ggmap::qmplot(mean_long, mean_lat , color = cluster %>% as.factor(), data = sightings ) + theme_classic()


length_to_density_data <- length_to_density_data %>% 
  left_join(sightings %>% select(classcode, cluster), by = 'classcode')

all_sightings <- length_to_density_data %>% 
  filter(is.na(total_biomass_g) == F & total_biomass_g > 0) %>% 
  select(classcode, commonname, site, side, cluster) %>% 
  left_join(site_coords, by = c('site','side')) %>% 
  filter(is.na(latitude) == F & is.na(longitude) == F)  %>% 
  left_join(site_data %>% select(site, side, region), by = c('site','side'))

qmplot(longitude,latitude, color = cluster %>% as.factor(), data = all_sightings) + theme_classic()

all_sightings %>% 
  group_by(cluster,region) %>% 
  summarise(nobs = length(site)) %>% 
  ungroup() %>% 
  arrange(nobs)


```



# 2017-12-21 chat with brandon

OK, so a important thigns came out of that conversation with Brandon. The core is: you should try and just back out the net MPA effect directly from the model, instead of in this hierarchichal way. It should give you way more power to detect the MPA effect. You're already extracting the "net" year effect, so just do the EXACT SAME DAMN THING except hold year constant and change the MPA/targeted effect as needed. 

That a) gives you waaaaay more data points to estimate that thing, holding other crap constant, and b) would allow you to estimate the effect by species, which degrees of freedom don't allow you to do as it currently stands with targeted and non-targeted... but it's something to think about. 

So, the next step is to 1) finish what you were doing on the current method (add in species intercepts) and run that thing.

Then, build a new model in TMB for speeds sake that does this step, being careful to make sure that it's still the MPA effect that you're estimating. 

Compare and see what happens. 

Part of the reason for the hierarchihcal stuff normally is that you're trying to estiamte things you can't really observe, and so you're imposing structure to get at that. i.e. priors on random effects in a state space sense that otherwise wouldn't be identifiable. 

# 2018-1-2 Diagnosing problem variables

The damn stan version of the TMB file finally ran.... Took absolutely forever and not entirely clear if you even got the same results. So, let's do a few things. Let's try and see if you actually got the same results first. If you did, then let's try and figure out what the problem variables are

Results look pretty damn similar so that's good. 

Problem variables 

- 2002_seen (sort of)

- 2004_seen 

- 2006_seen

- 2008 seen

-2009_seen

- 2010_seen

- 2011_seen

-2012_seen

Basically, a whole bunch of the did parameters had a really hard time mixing

- factor_year-2002_seen

- factor_year-2004_seen

- factor_year-2003_seen

Basically all of the year fixed effects cause major problems

intercept seen

targeted_seen

THe year-species effects have problems too

cpri classcode in particular.. cpri is a nigthmare actually... huh. That's ocean whitefish. Aha, which is also the least actually observed of the species that make it in... only 300 observations in total. 

cpun is the same, reallly bad. though interestingl that's one of the most observed species. What the hell? yeah cpun is a disaster. 

Almost all the other covariates are great. It's really just trying to estimate all these year effects by species that is really fucking stuff up. 

So, the options. 1 is that you just need to give up on stan for this sort of thing, maybe it just doesn't work all that well for complex ecological problems. So, you go with TMB. 

THe other though is that maybe you should take this as a warning. There's a reason that the model is having a really hard time fitting all these things, and so even in TMB works, you are maybe really underestimating the uncertainty. So, solutions there. 

You can get rid of the year-species effects, and instead try and add in more species-environment interactions , like enso and pdo by month, and see if getting rid of those helps you out with the problem of getting all these years dialed in. That has the advantage of making life a little better at interpreting the variables. 

The other thing I want to check is the effective sample size


# 2018-01-03 

OK, TMB works just fine, but lord that result is sensitive to the thigns that you include in there.

To get Stan to work, you'll probably have to experiment with non-centering the random effects. Let's go back to that at another occasion. 

Rather than just poking at things randomly, it's time to pull things back together and make some decisions about where to go from here. 

Right off the bat, I think you have to ditch the "raw" vs "length to density" thing. It's good to have it in your back pocked saved, but makes life needless complicated and doesn't really stand up to scrutiny: reults only hold if we use a somewhat esoterically processed version of the raw data. 

Now, the KFM data are interesting, but present a bit of a challenge for automation since they are missing a whole pile of other covariates, but seems like should be included if possible... let's revisit that. 

For now though, I think you should focus on doing the best job possible with the "length to density data", and maybe even changing that stupid ass name. Then, once you design that as well as possible, you can revisit the other data, namely the KFM data. 

The other important question is what to do with all the glm/glmer crap. Is there any value in it? You could certainly use it as the basis for a bootstrapped version of either the two stage or one stage method, so seems worth keeping around to some extent. They also serve a purpose in being much easier for model selection and comparison. They provide you some insight into how stable particular results are (not very). But, every model run now produces something like 400 graphs all showing different things, which isn't really all that tractable, and the broken out models don't help you actually get to the point you want, without bootstrapping. So why don't you keep those, and write a new function that takes a delta model and either 1) extracts years and fits did or 2) does the two stage thing... Seems like backburner priority right now over getting the integrated version incorporated. 

What are the key features that you want to explore right now?

One is clearly model selection: what's in and what's out. Looking at the TMB outputs though, nearly all of the covariates you include are significant, and the ones that aren't are just components of some factor. So, there actually doesn't seem like all that much room for model selection at this point: hard to justtify removing observer experience or something like that. 

But, there is some question of highly collinear variables... could consider picking 1 or 2 of them.... should read up on this some. It's hard to think too causally about a parameter if it is simply absorbing another parameter, but it's worth thinking about. 


Another big question is data weighting.... do you want to weight each observation equally, or each species equally? Given the imbalance in the samples, it seems like the DiD term is really the effect of the most common targeted species to some extent

Another is whether the random effects are kosher. They certainly help a lot, since the DiD term as I interpret is the common deviation in the trends attributable to being a targeted species, beyond the general trend for that species. I like that. But, it's a little unclear whether it's kosher or not, in terms of causal interpretation. 

You can also explore treating them as fixed effects.... though it makes things a little more complicated since implies that the did term is relative to one species in one year in particular... right? revisit that. 

Another is whether there is one population or many. If you're (somehow) including year terms, then it's just a question of adding more of them in (ugh). I think you run into sample size issues pretty quickly there. 

You do want to be able to do MPA or no MPA. Can do either by filtering and fitting separately, or could think about adding in effects and then measuring join and separate MPA effects.... mess with that. 

So what to actually do here. Let's go through and strip the fat out of run-ahnold, maybe breaking a few things into functions that more cleanly spit out the actual data that you care about. 

Then, let's add in the old stuff as an optional section, just to keep it alive and well without the supplied density shite. 

Then, I want to focus on a thing that runs the TMB model a few times: straight up DiD, random effect version, and wildcard. Do that all data, and broken out inside/outside MPA. 

Focus on those are your primary results, and then tinker from there. But the goal: this is DONE tomorrow. Then, Friday/weekend is spent on paper draft

VAST is interesting.... should also keep that capability in there

HOLY SHIT. SEE GELMAN MULTILEVEL DATA ANALYSIS 23.2

YOU'VE been waaaaaaay to married to the exact DiD specification. You're essentially doing it manually you dumbass. 

Consider a case where you include time trends by species. That takes care of each species intercepts. You then also include a trend of targeted over time. Now, your hypothesis is that targeted species were affected by the policy. So, if that's true, then on average targeted species should start to diverge a bit. THey all have their same general trend, but if it's correct, the model would parse a bit of the trend out into the group level predictor of the targeted by time. 

Now, you use those coefficients to predict the response. HOlding everything else constant, you then get the change in predicted density as a function of that thing. But, you've got all that other noise in there, so hold the treatment constant as well. 

So, if this is right, the simplest form of the DiD that you can think of here is random effects by species over time, fixed effects for being targeted in each year, dropping the intercept so keeping all terms. Then, manually do the DiD in the code as you are currently doing. Let's test this out tomorrow before launching into it, but I think this has promise. 

# Setting up the simulated data

You need to stop randomly poking at regression structures until something works. You need to do two things


1. Actually sit down with the fucking math, or try to

2. Really dive into the simulation exercise. Use SPASM. Simulate a few species with a common environmental forcing where each species responds a little differently to that forcing. Simulate a few observers, each with a different skill that evolves over time (selectivity and q and prob seen), where the probability seen is a linear function of density, with recruitment deviates drawn from a common random walk function. The hard part is what to do with the zeros.... If there is a true density and if you see it you sample from that and if they are there but you just don't see it, then if what you care about is a sample of the density then that's fine... right?

This is the really hard part. Think about Jason's example. Suppose that you send out your samples and come up with 99 zeros and one positive. If you only look at the positive, your estimate will be biased, if the next year for example the population increases but you get a 50 observations but lower than the huge school you saw before. The problem is, are you trying to estimate the transect density or the population density? is that right? 

Think about the data. You get a bunch of transects, some positive some zero. Suppose that every transect actually had 50 fish. then your estimate of the density is biased. Supposed though that some really had zero fish. Then, if you had to estiamte the density of the *average transect*, then you need to include them. And that's what you're really trying to estimate. So, suppose that probability of detection scales with population size. So, as you get more and more transects filled in, then your density of the average transect should get better and better. But, the other problem is true zeros. The example that you just went through is basically saying that your estimate of true density gets less biased with higher population. But, what if there is just more of a chance of true zeros at lower population sizes? How do you simulate that? 

Here's what I think. You treat the thing as one population. Every time step, you distribute the population as you already do. Then though, you set up a series of transects in each patch. Those transects either contain or do not contain fish in proportion to the abundance in the patch that transect occurs in, say through some logistic function where as the population gets bigger the probability of a transect having that species goes to 1..... too complicated. ugh.

Let's start without the observation side: just the premise that you should be able to estimate the MPA affect and control for environmental noise using the DiD. If that works, then you'll tackle the added pain in the ass of detection probabilities. 

So what screws with DiD. DiD assumes parallel trends and constant differences among groups. So, let's do one simulation where all populations are at EQ, no recruitment violations, etc. The only change is the MPA, and use the model to estimate that difference. ALl fished species are equally overfished. See if the model can tackle that. 

Then, let's fuck with things. Add in a recruitment deviate function that is related to ENSO, where it's good for some species and bad for others, and feed it the enso data. Change the degree of overfishing for each species, in proportion to the magnitude of the catches for each species from the data, why not. That should cause some violation of parallel trends and make the effect size heterogeneous. And, seems like the a good approximation of the two most simple violations of the DiD assumptions. The predator/prey thing is straightforward to think about generally. 

You then want to compare the ability of the different models to nail both of these. 

To generate data from this system, in each time step you'll generate a bunch of samples using your observers. ugh. 

Once you've found the best one for that, you'll tackle the observation thing. 

I suppose one option is to 

Simulate them all forward, targeted and non-targeted, and calculate the effect of the MPAs (the difference in log density with and without MPA over time)

Calculate the density in each cell, sample the age structure 

# How to simulate visual survey?

At the moment, the model is trying to estimate the average change in transect density as a function of covariates and the mpas. Right now, that sampling event is a transect. So, the density of the average transect should go up by blah. 

Now, it may well be that the transect level is just too high a variance event to be included in this analysis, but that's a data problem, not a simulation testing problem. 

Taking a sampling event to happen in one cell, how do you want to think about that process. Under that idea, there are 100 fish in a box, and you send a diver down to sample them. One option then is to treat it just as a fishing problem, where each diver "catches" fish in proportion to effort, q, and selectivity. This has the advantage of being easy to implement, and deals with the density proportional to abundance thing. The downside to this is that it would imply that you always the same proportions of each fish, i.e. every sampling event you just see the selectivity ogive, which isn't really accurate. 

So, a multinomial process makes more sense on that level. The idea there is that you are more or likely to see fish in a certain length group. i.e. if you sample 100 fish from the catch, you would sample them with a net proportion of the selectivity and their relative abundance in the population. But, that leaves the annoying question of how many fish to sample. In theory you want that process to scale with abundance..... 

AHA, this is how you do it. Catch, sum the total catch in numbers, then distribute that total catch in numbers using rmultinom in proportion to each age classes contribution to the catch. So, the "error" there comes from observation, meaning you had a chance to see a bigger fish but you actually didn't see it. This is still imperfect, since it doesn't really introduce stochasticity into the count process itself.... i.e. you'll always see 100 fish, it will just change the distribution of those 100 fish. 

What I really want is something that introduces noise in both the number seen and the distribution seen. But let's put a pin in that for now, and revisit that later. This should get you more or less where you want off sampling biomass with noise, where the q/effort/selectivity influence the ability of the observer. For a given effort, the higher the q, the higher the sample size, and therefore the higher resolution your samples of the age distribution will be. 

# 1/18 Check in meeting with Chris and Steve

Value of information in terms of what things are really driving the bus in teh detection 

Start with MSE

Show that under simulated you can't detect it

Verify that in an empirical situation

# 2018-1-26 personal check in

OK, that got a little out of hand. I clearly completely forgot what I'd done before the last meeting, so let's take stock. 

You've basically abandoned the whole F*ING structure you built before, of passing and testing arbitrary combinations of variables, data structures, etc, for a few good reasons. 

  - There's no good reason to keep dealing with jenn's processed data, it's obvious that your approach gets there and if anything it fixes a bug or two
  - the kfm data is interesting but so limited that being able to incorporate that is really the least of your worries right now
  - There is no good way to knit the two regression models together. You could build some bootstrapping approach, but that sounds like a pain in the ARSE
  - You were getting such conflicting results from that approach it's not like doing that would solve anything
  
To that end, you now have `run-ahnold-3.R`, which is your new wrapper, built mostly around producing the `TMB` fits. Though there is some rough legacy built around running the old version if you want, but I doubt that will work much longer. 

The TMB fits allow for pretty easy switching among scripts. You have two versions of this, `fit_ahnold.cpp`, which is the standard approach of incorporating everything into two regressions, and then backing out the MPA effect as a derived quantity. 

There is also `fit_ahnold_two_stage.cpp`, which does the standardization-then-fit process (though you haven't tested that one in a while, unclear how well it still works)

You also have identically names .stan files, which in theory run the stan version of this, but caveat emptor on those, since they seem to take for absolute ever. 

So, that produces your most up-to-date "results", such as they are. I think you need go back in there and improve the saving process on those. 

So, that all led you back to the simulation results, which live in `sim-mpa`. 

I truly have no idea what you were doing before to get those faster convergences, but you clearly got drunk and changed something since it takes for ever now unless you cap max_treedepth. But, it seems to sort of work again. so, `sim-mpa` is where you now do your simulation test to show that unless you explicitly control for the environmental stuff this whole thing breaks down pretty badly. 


