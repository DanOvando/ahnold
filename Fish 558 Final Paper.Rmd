---
title: "MLPA Effects"
author: "Dan Ovando"
date: "December 4, 2015"
output: html_document
csl: fish-and-fisheries.csl
bibliography: MLPA Effects.bib
---

```{r, include=F}
knitr::opts_chunk$set(fig.path='Figs/', echo=FALSE, warning=FALSE, message=FALSE)

```

```{r load libraries}
library(knitr)
library(gridExtra)
library(ggplot2)
library(plyr)
library(dplyr)
library(tidyr)
library(broom)
library(coda)
library(ggmcmc)
library(LaplacesDemon)
library(foreach)
library(scales)
library(stargazer)
library(DT)
library(ggmap)
library(texreg)
# library(VGAM)
library(AER)
library(msm)
library(mvtnorm)
devtools::load_all('MLPAFuns')
```

```{r run options}

runfolder <- 'Static 1.0 '

runpath <- paste('MLPA Effects Results/',runfolder,'/', sep = '')

if (dir.exists(runpath) == F)
{
  dir.create(runpath, recursive = T)
}


```


## Introduction

This is the blank space for your final paper

## Methods

### Data Processing

* What's an MPA?

Currently your treatment is actually "MPA", which is a bit wrong. So in the case of Anacapa, there's no "pre" period, and you have like 35 years of being an MPA. 

The better appraoch: treatment should be MLPA. So, anacapa in 2000 should be FALSE for has_mpas, and the years MPA should be the years since an MLPA mpa went into place

* What's filtered

* Missing Data


### The Model Model Construction

$$ d_{f,s,y} = \beta_{1}fished_{f} + \beta_{2}mpa_{s,y} + \beta_{3}FxM_{f,s,y} +
\sum\beta_{4}region_{s} + \sum\beta_{5}trophic_{f} +  \beta_{6}yearsmpa_{s,y} +... $$ 

$$\beta_{7}FxYM_{f,s,y} + \beta_{7}linf_{f} + \beta_{8}vbk_{f} + \beta_{9}temp_{s,y} +
\beta_{10}vis_{s,y} + $$ 

$$ \beta_{11}templag1_{s,y} + \beta_{12}templag2_{s,y} + \beta_{13}templag3_{s,y} + 
\beta_{14}templag4_{s,y} + \beta_{15} $$




### Likelihoods

Given the zero-truncated nature of the data, and the lack of apparent truncated normality in the data, we choose to model the data using a hierarchical delta-normal GLM approach. Under this approach, the data has to cross a "hurdle", in which we model the likelihood of a given observation being greater than 0, and then the likelihood of the positive density data conditional on the likelihood of being greater than 0. Following Babcock & McAllister [-@Babcock2002], we can break the likelihood into two components, a binomial process modeling the likelihood of the observed densities being greater than 0, and a normal likelihood for the distribution of the log-trasformed densities. 

### Binomial distribution

For a given aggregation level, e.g. at the species, site-side, year level currently adopted, we can tally the number of candidate transects *n* and the number of transects with positive densities *w*. This translates to a binomial process where the likelihood of a given aggregation is given as

$$ LL^{bin}_{f,s,y} = dbinom(w_{f,s,y},n_{f,s,y},p_{f,s,y}) $$

where *p* is the estimated probability of the the number of positive densities. *p* is estimated through a logit regression of the form 

$$ *p_{f,s,y}* = \frac{e^{\beta_{f,s,y}...}}{1+e^{\beta_{f,s,y}...}} $$

where $\beta$ is a linear regression of predictor variables. 

### Normal distribution

For the positive densities... normal distribution. s


### Priors

The default for now are uniform priors. 

There are a couple odd levels of aggregation in the data. The only thing that really changes
at the species-siteside-year level are *mpa_applied*, *fished_x_mpa*,*fished_x_yearsmpa*. 

mpa_applied operates at a regional level 

## Results


You can reproduce most of the diagnostics from the Mosquito exercise. For now, let's focus on observed vs predicted to see if you're even in the same ballpark

```{r assess MCMC fit}

# load(paste(runpath,'MCMC Results.Rdata', sep = ''))
# 
# dat <- bayes_reg$Data
# 
# post <- bayes_reg$demon_fit$Posterior1
# 
# its <- dim(post)[1]
# 
# burn <- 0.5
# 
# trimmed_post <- thin_mcmc(post[(burn*its):its,], thin_every = its/1000)
# 
# ggmcmc(ggs(mcmc(trimmed_post)), file = paste(runpath,'mcmc diagnostics.pdf', sep = ''))
# 

```


### Diagnostis

## Discussion 



